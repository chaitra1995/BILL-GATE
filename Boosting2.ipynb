{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a73bbf-2511-454c-a9d2-78e26497890b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is Gradient Boosting Regression?\n",
    "\n",
    "Gradient boosting regression trees are based on the idea of an ensemble method derived from a decision tree.\n",
    "The decision tree uses a tree structure. Starting from tree root, branching according to the conditions and\n",
    "heading toward the leaves, the goal leaf is the prediction result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4b773e-e6b1-4f39-9d44-be9ed521b8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple \n",
    "regression problem as an example and train the model on a small dataset. Evaluate the model's performance\n",
    "using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5126ad2f-3179-4268-96b4-2cefc3724b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error,make_scorer\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
    "\n",
    "class GradientBoostTreeRegressor(object):\n",
    "    #initialiser\n",
    "    def __init__(self, n_elements : int = 100, max_depth : int = 1) -> None:\n",
    "        self.max_depth     = max_depth\n",
    "        self.n_elements    = n_elements\n",
    "        self.f             = []\n",
    "        self.regions       = []\n",
    "        self.gammas        = []\n",
    "        self.mean_loss     = []\n",
    "        self.e0            = 0\n",
    "        \n",
    "    #destructor\n",
    "    def __del__(self) -> None:\n",
    "        del self.max_depth\n",
    "        del self.n_elements\n",
    "        del self.f\n",
    "        del self.regions\n",
    "        del self.gammas\n",
    "        del self.mean_loss\n",
    "        del self.e0\n",
    "\n",
    "#private function to group data points & compute gamma parameters\n",
    "    def __compute_gammas(self, yp : np.array, y_train : np.array, e : np.array) -> Tuple[np.array,Dict]:\n",
    "        #initialise global gamma array\n",
    "        gamma_jm = np.zeros((y_train.shape[0]))\n",
    "        #iterate through each unique predicted value/region\n",
    "        regions = np.unique(yp)\n",
    "        gamma   = {}\n",
    "        for r in regions:\n",
    "            #compute index for r\n",
    "            idx = yp == r\n",
    "            #isolate relevant data points\n",
    "            e_r = e[idx]\n",
    "            y_r = y_train[idx]\n",
    "            #compute the optimal gamma parameters for region r\n",
    "            gamma_r = np.median(y_r - e_r)\n",
    "            #populate the global gamma array\n",
    "            gamma_jm[idx] = gamma_r\n",
    "            #set the unique region <-> gamma pairs\n",
    "            gamma[r] = gamma_r\n",
    "        #append the regions to internal storage\n",
    "        self.regions.append(regions)\n",
    "        #return\n",
    "        return((gamma_jm,gamma))\n",
    "\n",
    "#public function to train the ensemble\n",
    "    def fit(self, X_train : np.array, y_train : np.array) -> None:\n",
    "        #reset the internal class members\n",
    "        self.f             = []\n",
    "        self.regions       = []\n",
    "        self.model_weights = []\n",
    "        self.mean_loss     = []\n",
    "        #initialise the ensemble & store initialisation\n",
    "        e0      = np.median(y_train)\n",
    "        self.e0 = np.copy(e0)\n",
    "        e       = np.ones(y_train.shape[0]) * e0\n",
    "        #loop through the specified number of iterations in the ensemble\n",
    "        for _ in range(self.n_elements):\n",
    "            #store mae loss\n",
    "            self.mean_loss.append(np.mean(np.abs(y_train - e)))\n",
    "            #compute the gradients of our loss function\n",
    "            g = np.sign(y_train - e)\n",
    "            #initialise a weak learner & train\n",
    "            model = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            model.fit(X_train,g)\n",
    "            #compute optimal gamma coefficients\n",
    "            yp             = model.predict(X_train)\n",
    "            gamma_jm,gamma = self.__compute_gammas(yp,y_train,e)\n",
    "            #update the ensemble\n",
    "            e += gamma_jm\n",
    "            #store trained ensemble elements\n",
    "            self.f.append(model)\n",
    "            self.gammas.append(gamma)\n",
    "            \n",
    "\n",
    "#public function to generate predictions\n",
    "    def predict(self, X_test : np.array) -> np.array:\n",
    "        #initialise predictions\n",
    "        y_pred = np.ones(X_test.shape[0]) * np.copy(self.e0)\n",
    "        #cycle through each element in the ensemble\n",
    "        for model,gamma,regions in zip(self.f,self.gammas,self.regions):\n",
    "            #produce predictions using model\n",
    "            y = model.predict(X_test)\n",
    "            #cycle through each unique leaf node for model m\n",
    "            for r in regions:\n",
    "                #updates for region r\n",
    "                idx = y == r\n",
    "                y_pred[idx] += gamma[r] \n",
    "        #return predictions\n",
    "        return(y_pred)\n",
    "           \n",
    "    #public function to return mean training loss\n",
    "    def get_loss(self) -> List:\n",
    "        return(self.mean_loss)\n",
    " \n",
    "    #public function to return model parameters\n",
    "    def get_params(self, deep : bool = False) -> Dict:\n",
    "        return {'n_elements':self.n_elements,\n",
    "                'max_depth':self.max_depth}\n",
    "\n",
    "\n",
    "fX,sy = load_diabetes(return_X_y=True,as_frame=True)\n",
    "rgr = GradientBoostTreeRegressor(n_elements=100)\n",
    "rgr.fit(dfX.values,sy.values)\n",
    "\n",
    "loss1 = rgr.get_loss()\n",
    "rgr = GradientBoostTreeRegressor(n_elements=100, max_depth=2)\n",
    "rgr.fit(dfX.values,sy.values)\n",
    "\n",
    "loss2 = rgr.get_loss()\n",
    "rgr = GradientBoostTreeRegressor(n_elements=100, max_depth=3)\n",
    "rgr.fit(dfX.values,sy.values)\n",
    "\n",
    "loss3 = rgr.get_loss()\n",
    "rgr = GradientBoostTreeRegressor(n_elements=100, max_depth=4)\n",
    "rgr.fit(dfX.values,sy.values)\n",
    "\n",
    "loss4 = rgr.get_loss()\n",
    "\n",
    "plt.plot(loss1,label='max_depth=1')\n",
    "plt.plot(loss2,label='max_depth=2')\n",
    "plt.plot(loss3,label='max_depth=3')\n",
    "plt.plot(loss4,label='max_depth=4')\n",
    "plt.title('Training Loss by Boosting Iteration')\n",
    "plt.xlabel('Number of Component Trees')\n",
    "plt.ylabel('MAE Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "scoring_metrics = {'mse' : make_scorer(mean_squared_error), \n",
    "                   'mae': make_scorer(mean_absolute_error)}\n",
    "\n",
    "\n",
    "rgr = GradientBoostTreeRegressor(n_elements=20,max_depth=1)\n",
    "#cross validate\n",
    "dcScores = cross_validate(rgr,dfX.values,sy.values,cv=10,scoring=scoring_metrics)\n",
    "#report results\n",
    "print('Mean MSE: %.2f' % np.mean(dcScores['test_mse']))\n",
    "print('Mean MAE: %.2f' % np.mean(dcScores['test_mae']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaabf881-4d67-4bdb-b420-d5f09609712d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth\n",
    "to optimise the performance of the model. Use grid search or random search to find the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96b616df-a5ef-4ca0-bba3-1405ff748601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 12, 'max_features': 1}\n",
      "0.8582251082251082\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "df = pd.read_csv('dataset.csv')\n",
    "X = df.drop('target', axis = 1)\n",
    "y = df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)\n",
    "rfc = RandomForestClassifier()\n",
    "forest_params = [{'max_depth': list(range(10, 15)), 'max_features': list(range(0,14))}]\n",
    "clf = GridSearchCV(rfc, forest_params, cv = 10, scoring='accuracy')\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.best_params_)\n",
    "print(clf.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921b8ac8-55e2-4157-b0ae-93c31eeaafe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What is a weak learner in Gradient Boosting?\n",
    "\n",
    "The term Weak Learner refers to simple models that do only slightly better than random chance.\n",
    "Boosting algorithms start with a single weak learner (tree methods are overwhelmingly used here),\n",
    "but technically, any model will do.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ddd595-0908-466b-8df2-07358c4f1b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "\n",
    "Gradient boosting is a type of machine learning boosting. It relies on the intuition that the best possible \n",
    "next model, when combined with previous models, minimizes the overall prediction error. The key idea is to \n",
    "set the target outcomes for this next model in order to minimize the error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5e0d1e-d3e6-49d9-8c71-683fa5fb0ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "\n",
    "The gradient-boosting regressor works by iteratively building an ensemble of weak learners, where each\n",
    "subsequent weak learner is trained to correct the mistakes made by the previous ones. The predictions \n",
    "from all weak learners are combined to make the final prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed14579-547a-4b6b-bd9e-a563a6694755",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?\n",
    "The steps involved  in  constructing  the mathematical  intuition  of Gradient Boosting  algorithm\n",
    " \n",
    "\n",
    "Step1:\n",
    "It is the calculation of the average value which is defined as a constant value. We calculate the constant\n",
    "value or the first predicted Salary value by applying the concept of minimizing the difference between actual and\n",
    "predicted results i.e. Loss function. Mathematically minimizing anything means differentiation at that point is zero. \n",
    "Hence in the below image we\n",
    " \n",
    "\n",
    "Now putting back the values of y for each record we can calculate the predicted value or average value for\n",
    "the entire dataset.\n",
    " \n",
    "Step 2(a):\n",
    "For Ῡ = F₀( x ) = predicted value we need to now calculate the residual or difference of predicted value w.r.t y.\n",
    "To calculate residual we use the Decision Tree which is represented by m.\n",
    "rᵢₘ = residual of record I for decision tree m.\n",
    " \n",
    "Step 2(b):We now construct decision tree 1 using the residual as outputs and the features x as inputs.\n",
    "Step 2(c):For each actual residual output which is obtained in the above step, we will now calculate the predicted\n",
    "output Ῡ.\n",
    "We already have the predicted value of the model1 which is defined as the average value in step-1.\n",
    "To obtain the residual predicted output in the Decision Tree we are using the concept of loss function of step-1.\n",
    "j = leaf node in decision tree\n",
    "yᵢ = actual output\n",
    "Ῡ = predicted output\n",
    "Fₘ-₁( x ) = previous model output\n",
    " \n",
    "Step-2(d)\n",
    "After obtaining the predicted value of the Decision tree we combine it with the previous model value. \n",
    "The loop will keep on repeating itself until the predicted value is equal to the actual value.\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19834c6e-39c7-4312-9323-974398dd38d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7713b1de-0247-40cc-9831-747549e23b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is the KNN algorithm?\n",
    "\n",
    "The K-Nearest Neighbors (KNN) algorithm is a popular machine learning technique used for classification and \n",
    "regression tasks. It relies on the idea that similar data points tend to have similar labels or values.\n",
    "During the training phase, the KNN algorithm stores the entire training dataset as a reference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3cf418-98fb-4448-b956-d0acd5798422",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. How do you choose the value of K in KNN?\n",
    "\n",
    "The optimal K value usually found is the square root of N, where N is the total number of samples.\n",
    "Use an error plot or accuracy plot to find the most favorable K value. KNN performs well with multi-label\n",
    "classes, but you must be aware of the outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26a0f27-1bae-421e-a41f-d938c738b4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. What is the difference between KNN classifier and KNN regressor?\n",
    "Regression Algorithm\t                                 Classification Algorithm\n",
    "1.In Regression, the output variable must be \n",
    "of continuous nature or real value.\t               1.In Classification, the output variable must be\n",
    "                                                              a discrete value.\n",
    "2.The task of the regression algorithm is to map\n",
    "the input value (x) with the continuous\n",
    "output variable(y).\t                             2.The task of the classification algorithm is to map\n",
    "                                                 the input value(x) with the discrete output variable(y).\n",
    "3.Regression Algorithms are used\n",
    "with continuous data.\t                         3.Classification Algorithms are used with discrete data.\n",
    "4.In Regression, we try to find the best fit line,\n",
    "which can predict the output more accurately.\t  4.In Classification, we try to find the decision boundary,\n",
    "                                                which can divide the dataset into different classes.\n",
    "5.Regression algorithms can be used to solve \n",
    "the regression problems such as Weather Prediction, \n",
    "House price prediction, etc.\t                 5.Classification Algorithms can be used to solve\n",
    "                                                classification problems such as Identification of spam emails,\n",
    "                                                 Speech Recognition, Identification of cancer cells, etc.\n",
    "6.The regression Algorithm can be further \n",
    "divided into Linear and Non-linear Regression.\t6.The Classification algorithms can be divided into\n",
    "                                                    Binary Classifier and Multi-class Classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ddbfb8-d33d-41e4-89bd-1ea246f78fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. How do you measure the performance of KNN?\n",
    "\n",
    "Two common approaches are cross -validation and training/testing split. Overall, both cross -validation\n",
    "and training/testing split are useful approaches for evaluating the performance of the KNN algorithm,\n",
    "and the choice between them depends on the specific problem and the available resources.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f851e27d-12ec-437a-adf0-fa366244f97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. What is the curse of dimensionality in KNN?\n",
    "\n",
    "The Curse of Dimensionality refers to the various challenges and complications that arise when analyzing \n",
    "and organizing data in high-dimensional spaces (often hundreds or thousands of dimensions).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4e01cd-75ae-409b-9778-fd25ab803a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. How do you handle missing values in KNN?\n",
    "1. Retains Data: KNN Imputer retains the most data compared to other techniques such as removing rows\n",
    "or columns with missing values. It replaces missing values with imputed values, ensuring that the \n",
    "entire dataset is used for analysis and modeling.\n",
    "2. Preserves Relationships: KNN Imputer imputes missing values based on the nearest neighbors, \n",
    "which means it preserves the underlying relationships in the data. It takes into account the feature\n",
    "similarities between data points to estimate the missing values, making it more contextually relevant.\n",
    "3. Non-parametric: KNN Imputer is a non-parametric method, which means it does not make assumptions \n",
    "about the data’s distribution. It is suitable for both numeric and categorical data, making it versatile\n",
    "in handling various types of missing values.\n",
    "4. Flexibility: The KNN Imputer allows you to specify the number of nearest neighbors to use for \n",
    "imputation (controlled by the n_neighbors parameter). This flexibility lets you adapt the imputation \n",
    "to the specific characteristics of your dataset.\n",
    "5. Less Bias: By using multiple neighboring data points to estimate missing values, KNN Imputer\n",
    "reduces the bias that may be introduced when using simple imputation techniques like mean or median imputation.\n",
    "6. Improved Accuracy: In scenarios where missing values are not entirely at random, KNN Imputer\n",
    "can perform better than traditional imputation methods, leading to more accurate models and analyses.\n",
    "7. Easy Implementation: Scikit-Learn provides a user-friendly implementation of the KNN Imputer, \n",
    "making it easy to incorporate into your data preprocessing pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e98d403-d874-4f5a-b764-ef8efbad7327",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. Compare and contrast the performance of the KNN classifier and regressor.\n",
    "Which one is better for which type of problem?\n",
    "\n",
    "KNN regression tries to predict the value of the output variable by using a local average.\n",
    "KNN classification attempts to predict the class to which the output variable belong by computing\n",
    "the local probability.Regression algorithms solve regression problems such as house price predictions and\n",
    "weather predictions. Classification algorithms solve classification problems like identifying spam e-mails,\n",
    "spotting cancer cells, and speech recognition.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823c4c50-495c-4188-8b1a-282948959896",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks,\n",
    "and how can these be addressed?\n",
    "•\t1 Advantage 1: Easy to implement and understand. ...\n",
    "•\t2 Advantage 2: Lazy learning and no assumptions. ...\n",
    "•\t3 Disadvantage 1: High memory and computational cost. ...\n",
    "•\t4 Disadvantage 2: Choice of k and distance metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa4ed50-1cd8-4fe4-9507-f30a70769d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?\n",
    "The distortion in k-means using Manhattan distance metric is less than that of k-means using Euclidean \n",
    "distance metric. As a conclusion, the K-means, which is implemented using Euclidean distance metric gives\n",
    "best result and K-means based on Manhattan distance metric's performance, is worst.\n",
    "In the k-nearest neighbor (KNN) algorithm, the distance between points is used to determine which points\n",
    "are \"nearest\" to a given point. There are different ways to measure the distance between points, and two\n",
    "of the most common are the Euclidean distance and the Manhattan distance.\n",
    "\n",
    "The Euclidean distance is the most commonly used distance measure and is the one that is used by default \n",
    "in many KNN implementations. It is defined as the square root of the sum of the squares of the differences\n",
    "between the coordinates of the two points. It is a straight-line distance between two points, similar to\n",
    "the distance you would travel if you were to draw a line between the two points on a map.\n",
    "\n",
    "The Manhattan distance, also known as the \"taxi cab\" distance, is defined as the sum of the absolute\n",
    "differences of the coordinates of the two points. It is the distance you would travel if you were to \n",
    "follow a grid pattern, only moving horizontally or vertically, to get from one point to the other.\n",
    "Which distance measure you should use depends on the nature of the problem you are trying to solve. \n",
    "In general, the Euclidean distance is more sensitive to differences in the coordinates of the points \n",
    "and may be more appropriate when the points are spread out in a continuous space. \n",
    "The Manhattan distance is less sensitive to differences in the coordinates of the points and may be\n",
    "more appropriate when the points are organized in a grid-like pattern.\n",
    "\n",
    "It's worth noting that there are other distance measures that can be used in the KNN algorithm, and the \n",
    "choice of distance measure can have a significant impact on the performance of the model. Experimenting\n",
    "with different distance measures is usually a good way to find the one that works best for a particular problem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ec65ae-7497-4337-9df1-e7c450f4f4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q10. What is the role of feature scaling in KNN?\n",
    "Feature scaling is a method used to normalize the range of independent variables or features of data.\n",
    "In data processing, it is also known as data normalization and is generally performed during the data \n",
    "preprocessing step.KNN makes decision by identifying votes from the training datapoints that are nearest \n",
    "to the test datapoint (majority voting based on the k value). In such situation, scale of the features does \n",
    "matter. Variables with larger scale will have larger effect on the distance between observations and also \n",
    "on KNN classifier as compare to the variables on smaller scale.We need to do feature scaling first, \n",
    "so that they are on same scale to deal with this issue!We also know that Scikit-learn has built-in \n",
    "functionality to do feature scaling. We need to import StandardScalerfrom Scikit-learn and also need to \n",
    "create a StandardScaler() object (e.g. scaler). (try MinMaxScaler yourself and compare your results)\n",
    "the object scaler is now a fitted object on the features. We can use this scaler object to transform all \n",
    "features in the dataset. Scikit-learn provides .transform() method to do standardization job by centring \n",
    "and scaling the features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1f0270-8a4f-4115-bde3-eb93217ad960",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

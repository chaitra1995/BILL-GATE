{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc3a591-75df-48ee-96ef-af4200d11d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is the main difference between the Euclidean distance metric and the Manhattan distance metric in KNN?\n",
    "How might this difference affect the performance of a KNN classifier or regressor?\n",
    "The “Euclidean Distance” between two objects is the distance you would expect in “flat” or “Euclidean” space; \n",
    "it’s named after Euclid, who worked out the rules of geometry on a flat surface. The Euclidean is often\n",
    "the “default” distance used in e.g., K-nearest neighbors (classification) or K-means (clustering) to find \n",
    "the “k closest points” of a particular sample point. The “closeness” is defined by the difference \n",
    "(“distance”) along the scale of each variable, which is converted to a similarity measure.\n",
    "This distance is defined as the Euclidian distance. It is only one of the many available options to\n",
    "measure the distance between two vectors/data objects. However, many classification algorithms, \n",
    "as mentioned above, use it to either train the classifier or decide the class membership of a test \n",
    "observation and clustering algorithms (for e.g. K-means, K-medoids, etc) use it to assign membership\n",
    "to data objects among different clusters. Mathematically, it’s calculated using Pythagoras’ theorem.\n",
    "The square of the total distance between two objects is the sum of the squares of the distances along\n",
    "each perpendicular co-ordinate.\n",
    "Manhattan Distance Metric:\n",
    "Manhattan Distance is the sum of absolute differences between points across all the dimensions.\n",
    "Manhattan distance is a metric in which the distance between two points is the sum of the absolute \n",
    "differences of their Cartesian coordinates. In a simple way of saying it is the total sum of the difference \n",
    "between the x-coordinates and y-coordinates. This Manhattan distance metric is also known as Manhattan length,\n",
    "rectilinear distance, L1 distance or L1 norm, city block distance, Minkowski’s L1 distance, taxi-cab metric,\n",
    "or city block distance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cdc056-4ea3-47bd-80a7-81fa0bb5b357",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. How do you choose the optimal value of k for a KNN classifier or regressor? What techniques can be\n",
    "used to determine the optimal k value?\n",
    "The optimal K value usually found is the square root of N, where N is the total number of samples. Use an\n",
    "error plot or accuracy plot to find the most favorable K value. KNN performs well with multi-label classes,\n",
    "but you must be aware of the outliers.\n",
    "One can use cross-validation to select the optimal value of k for the k-NN algorithm, which helps improve \n",
    "its performance and prevent overfitting or underfitting. Cross-validation is also used to identify the outliers\n",
    "before applying the KNN algorithm.\n",
    "Elbow Method. The Elbow Method is a widely used technique for selecting the optimal number of clusters, K, \n",
    "in K-means clustering. It helps determine the K value where the within-cluster sum of squares (WCSS) exhibits\n",
    "a significant reduction, forming an elbow-like shape in the plot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e480bb-e072-4a4d-b7fa-a5dd679dfb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. How does the choice of distance metric affect the performance of a KNN classifier or regressor?\n",
    "In what situations might you choose one distance metric over the other?\n",
    "The effectiveness of a distance metric for optimizing the k-nearest neighbors (k-NN) algorithm depends on the\n",
    "nature of the data and the problem at hand. No one distance metric suits all types of data; the choice can be \n",
    "influenced by the scale of variables, distribution of the data points, and the presence of outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2372286-ea7b-4250-b1a7-2933dea58801",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What are some common hyperparameters in KNN classifiers and regressors, and how do they affect the\n",
    "performance of the model? How might you go about tuning these hyperparameters to improve model performance?\n",
    "\n",
    "A k-nearest neighbors is algorithm used for classification and regression. It classifies a new data point by\n",
    "finding the k-nearest points in the training dataset and assigns it the majority class among those neighbors.\n",
    "Machine learning algorithms have hyperparameters that allow you to tailor the behavior of the algorithm to your \n",
    "specific dataset. Hyperparameters Tuning can improve model performance by about 20% to a range of 77% for all\n",
    "evaluation matrices. Hyperparameter tuning in k-nearest neighbors (KNN) is important because it allows us to\n",
    "optimize the performance of the model. The KNN algorithm has several hyperparameters that can significantly\n",
    "affect the accuracy of the model, such as the number of nearest neighbors to consider (k), the distance metric \n",
    "used to measure similarity, and the weighting scheme used to aggregate the labels of the nearest neighbors\n",
    "Required Libraries:\n",
    "•\tNumPy\n",
    "•\tPandas\n",
    "•\tScikit-learn\n",
    "•\tMatplotlib.\n",
    "Things to keep in mind when performing turning:\n",
    "1.    Understand the parameters: The main hyperparameter to tune in k-nearest neighbors is k, the number \n",
    "of neighbors to consider. Other parameters include distance metrics, weights, and algorithm types.\n",
    "2.    Select a distance metric: Choose the right distance metric to measure the similarity between the data\n",
    "points. Common distance metrics include Euclidean, Manhattan, and cosine distance.\n",
    "3.    Select an appropriate value for k: Selecting a value for k is crucial in k-nearest neighbors. \n",
    "A larger value of k provides a smoother decision boundary but may not be suitable for all datasets.\n",
    "A smaller value of k may lead to overfitting.\n",
    "4.    Choose an algorithm type: k-nearest neighbors has two algorithm types: brute-force and tree-based.\n",
    "Brute-force algorithm computes the distances between all pairs of points in the dataset while tree-based \n",
    "algorithm divides the dataset into smaller parts.\n",
    "5.    Cross-validation: Cross-validation is a technique used to validate the performance of the model.\n",
    "It involves splitting the dataset into training and testing sets and evaluating the model's performance on the testing set.\n",
    "6.    Grid search: Grid search is a hyperparameter tuning technique that involves testing a range of \n",
    "values for each hyperparameter to find the best combination of values.\n",
    "7.    Random search: Random search is another hyperparameter tuning technique that randomly selects a\n",
    "combination of hyperparameter values to test.\n",
    "8.    Bias-variance tradeoff: k-nearest neighbors is prone to overfitting due to the high variance in the model. \n",
    "Regularization techniques such as L1 and L2 regularization can be used to mitigate overfitting.\n",
    "9.    Data preprocessing: Data preprocessing plays a crucial role in k-nearest neighbors. Scaling the data using\n",
    "techniques such as normalization and standardization can improve the model's performance. Outlier removal and\n",
    "feature selection can also help improve the model's performance.\n",
    " \n",
    "Hyperparameter tuning in k-nearest neighbors using Grid Search:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {'n_neighbors': np.arange(1, 11),\n",
    "              'weights': ['uniform', 'distance'],\n",
    "              'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "              'p': [1, 2]}\n",
    "\n",
    "# Define the KNN classifier\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Define the grid search object\n",
    "grid = GridSearchCV(knn, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit the grid search object to the training data\n",
    "https://www.linkedin.com/redir/phishing-page?url=grid%2efit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters and corresponding accuracy\n",
    "print(\"Best Hyperparameters:\", grid.best_params_)\n",
    "print(\"Best Accuracy:\", grid.best_score_)\n",
    "\n",
    "# Train and evaluate the model with the best hyperparameters\n",
    "best_knn = KNeighborsClassifier(n_neighbors=grid.best_params_['n_neighbors'],\n",
    "                                 weights=grid.best_params_['weights'],\n",
    "                                 algorithm=grid.best_params_['algorithm'],\n",
    "                                 p=grid.best_params_['p'])\n",
    "best_knn.fit(X_train, y_train)\n",
    "y_pred = best_knn.predict(X_test)\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(\"Accuracy:\", accuracy)        \n",
    "In this example, we use the Iris dataset, split it into training and testing sets, define a hyperparameter grid,\n",
    "define a KNN classifier, define a grid search object with 5-fold cross-validation, and fit the grid search object\n",
    "to the training data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bc9db0-605f-49fa-b39c-f3c461dcf5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. How does the size of the training set affect the performance of a KNN classifier or regressor? \n",
    "What techniques can be used to optimize the size of the training set?\n",
    "In contrast, during model optimization, you either increase or decrease depth and width depending on your goals.\n",
    "If your model quality is adequate, then try reducing overfitting and training time by decreasing depth and width.\n",
    "Specifically, try halving the width at each successive layer.\n",
    "•\tThe distance function or distance metric used to determine the nearest neighbors.\n",
    "•\tThe decision rule used to derive a classification from the K-nearest neighbors.\n",
    "•\tThe number of neighbors used to classify the new example.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de917cdc-cd6b-4bef-85c8-636df201f3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. What are some potential drawbacks of using KNN as a classifier or regressor? How might you overcome\n",
    "these drawbacks to improve the performance of the model?\n",
    "KNN has some drawbacks and challenges, such as computational expense, slow speed, memory and storage issues for\n",
    "large datasets, sensitivity to the choice of k and the distance metric, and susceptibility to the curse of dimensionality.\n",
    "KNN provides no insight about the relative importance of each predictor. Another significant disadvantage of KNN,\n",
    "is that the algorithm is computationally intensive. Computational effort of the algorithm increases greatly as\n",
    "more predictors, p, are considered and when the number of training records increase.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

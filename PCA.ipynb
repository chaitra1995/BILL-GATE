{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b216878-cb57-45c2-8f40-c547e265b2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is the curse of dimensionality reduction and why is it important in machine learning?\n",
    "\n",
    "The Curse of Dimensionality in Machine Learning highlights challenges when dealing with high-dimensional \n",
    "data. It affects diverse domains, increasing computational demands and reducing model performance.\n",
    "Overcoming it involves feature selection, dimensionality reduction, and careful algorithm choices\n",
    "The curse of dimensionality basically refers to the difficulties a machine learning algorithm faces\n",
    "when working with data in the higher dimensions, that did not exist in the lower dimensions. \n",
    "This happens because when you add dimensions (features), the minimum data requirements also increase rapidly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c60767-af11-4f62-bc69-235c03e98be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. How does the curse of dimensionality impact the performance of machine learning algorithms?\n",
    "\n",
    "The curse of dimensionality in machine learning is defined as follows, As the number of dimensions or\n",
    "features increases, the amount of data needed to generalize the machine learning model accurately increases\n",
    "exponentially.\n",
    "the Curse of Dimensionality in Machine Learning highlights challenges when dealing with high-dimensional\n",
    "data. It affects diverse domains, increasing computational demands and reducing model performance.\n",
    "Overcoming it involves feature selection, dimensionality reduction, and careful algorithm choices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f2ce60-c64c-48d0-9b8b-42cf936f4252",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. What are some of the consequences of the curse of dimensionality in machine learning, and \n",
    "how do they impact model performance?\n",
    "\n",
    "The Curse of Dimensionality in Machine Learning highlights challenges when dealing with high-dimensional data. \n",
    "It affects diverse domains, increasing computational demands and reducing model performance. Overcoming \n",
    "it involves feature selection, dimensionality reduction, and careful algorithm choices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acbe5ec-af06-41a5-9622-0644ee67714e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. Can you explain the concept of feature selection and how it can help with dimensionality reduction?\n",
    "\n",
    "Feature selection is simply selecting and excluding given features WITHOUT changing them. Whereas Dimensionally\n",
    "Reduction transforms the features into a lower dimension.\n",
    "Dimensionality reduction, or dimension reduction, is the transformation of data from a high-dimensional space\n",
    "into a low-dimensional space so that the low-dimensional representation retains some meaningful properties of \n",
    "the original data, ideally close to its intrinsic dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866fa15a-6815-42ba-90e0-dc6b7af31001",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. What are some limitations and drawbacks of using dimensionality reduction techniques in machine learning?\n",
    "\n",
    "•\tData loss. Dimensionality reduction should ideally have no data loss, as data can be recovered. \n",
    "•\tInterpretability. ...\n",
    "•\tComputational complexity. ...\n",
    "•\tOutliers.\n",
    "•\t1 Missing Value Ratio. Suppose you're given a dataset. ...\n",
    "•\t2 Low Variance Filter. ...\n",
    "•\t3 High Correlation filter. ...\n",
    "•\t4 Random Forest. ...\n",
    "•\t5 Backward Feature Elimination. ...\n",
    "•\t6 Forward Feature Selectio n. ...\n",
    "•\t7 Factor Analysis. ...\n",
    "•\t8 Principal Component Analysis (PCA)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872e8400-a221-4377-a829-0107143982b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. How does the curse of dimensionality relate to overfitting and underfitting in machine learning?\n",
    "\n",
    "KNN is very susceptible to overfitting due to the curse of dimensionality. Curse of dimensionality also\n",
    "describes the phenomenon where the feature space becomes increasingly sparse for an increasing number of \n",
    "dimensions of a fixed-size training dataset.\n",
    "Data sparsity is one of the facets of the curse of dimensionality. Training a model with sparse data could\n",
    "lead to high-variance or overfitting conditions. This is because while training the model, the model has\n",
    "learnt from the frequently occurring combinations of the attributes and can predict the outcome accurately.\n",
    "The curse of dimensionality leads to underfitting the data. If you have all this additional information,\n",
    "but do not increase the number of data points, the the model will have a hard time making sense of unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e766a8bc-488f-4c26-aac9-0ddd347c6c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. How can one determine the optimal number of dimensions to reduce data to when using\n",
    "dimensionality reduction techniques?\n",
    "\n",
    "Dimensionality Reduction is a statistical/ML-based technique wherein we try to reduce the number of \n",
    "features in our dataset and obtain a dataset with an optimal number of dimensions.\n",
    "One of the most common ways to accomplish Dimensionality Reduction is Feature Extraction, wherein we \n",
    "reduce the number of dimensions by mapping a higher dimensional feature space to a lower-dimensional \n",
    "feature space. The most popular technique of Feature Extraction is Principal Component Analysis (PCA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaad2fc2-af85-4508-9270-c670c1d1a549",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3aecbc4-645f-42fb-9b34-80fd71c98b7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c49deee-978d-4354-8a05-e543357b9aa4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cb8fd2-bc9c-4966-8399-3871ba82b116",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is hierarchical clustering, and how is it different from other clustering techniques?\n",
    "Hierarchical clustering is a popular method for grouping objects. It creates groups so that objects within \n",
    "a group are similar to each other and different from objects in other groups. Clusters are visually represented \n",
    "in a hierarchical tree called a dendrogram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01fb646-5203-4cb6-be7f-c3285969428b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. What are the two main types of hierarchical clustering algorithms? Describe each in brief.\n",
    "Agglomerative: This is a \"bottom-up\" approach: Each observation starts in its own cluster, and pairs of clusters\n",
    "are merged as one moves up the hierarchy.\n",
    "Agglomerative clustering is one of the most common types of hierarchical clustering used to group similar \n",
    "objects in clusters. \n",
    "Agglomerative clustering is also known as AGNES (Agglomerative Nesting). In agglomerative clustering, each data\n",
    "point act as an individual cluster and at each step, data objects are grouped in a bottom-up method. Initially,\n",
    "each data object is in its cluster. At each iteration, the clusters are combined with different clusters until \n",
    "one cluster is formed.\n",
    "\n",
    "Divisive: This is a \"top-down\" approach: All observations start in one cluster, and splits are performed recursively\n",
    "as one moves down the hierarchy.\n",
    "Divisive hierarchical clustering is exactly the opposite of Agglomerative Hierarchical clustering. \n",
    "In Divisive Hierarchical clustering, all the data points are considered an individual cluster, and in every \n",
    "iteration, the data points that are not similar are separated from the cluster. The separated data points are \n",
    "treated as an individual cluster. Finally, we are left with N clusters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb9bd26-55ff-4aa6-a7cd-ab012624c181",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. How do you determine the distance between two clusters in hierarchical clustering, and what are the common\n",
    "distance metrics used?\n",
    "complete linkage hierarchical clustering, the distance between two clusters is defined as the longest distance\n",
    "between two points in each cluster. For example, the distance between clusters “r” and “s” to the left is equal\n",
    "to the length of the arrow between their two furthest points\n",
    "For most common hierarchical clustering software, the default distance measure is the Euclidean distance.\n",
    "This is the square root of the sum of the square differences. However, for gene expression, correlation distance\n",
    "is often used.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b029beaa-763b-4057-85e7-d3d8843fec1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. How do you determine the optimal number of clusters in hierarchical clustering, and what are some common \n",
    "methods used for this purpose?\n",
    "To get the optimal number of clusters for hierarchical clustering, we make use a dendrogram which is tree-like\n",
    "chart that shows the sequences of merges or splits of clusters. If two clusters are merged, the dendrogram will\n",
    "join them in a graph and the height of the join will be the distance between those clusters.\n",
    "The optimal number of clusters k is the one that maximize the average silhouette over a range of possible values \n",
    "for k (Kaufman and Rousseeuw 1990). The algorithm is similar to the elbow method and can be computed as follow:\n",
    "    Compute clustering algorithm (e.g., k-means clustering) for different values of k.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50f8a6e-22fe-4821-8101-32fa48968825",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. What are dendrograms in hierarchical clustering, and how are they useful in analyzing the results?\n",
    "A dendrogram is a diagram that shows the hierarchical relationship between objects. It is most commonly created \n",
    "as an output from hierarchical clustering. \n",
    "A dendrogram is a tree-structured graph used in heat maps to visualize the result of a hierarchical clustering\n",
    "calculation. The result of a clustering is presented either as the distance or the similarity between the clustered\n",
    "rows or columns depending on the selected distance measure.\n",
    "There are two ways to interpret a dendrogram: in terms of large-scale groups or in terms of similarities among\n",
    "individual chunks. To identify large-scale groups, we start reading from the top down, finding the branch points\n",
    "that are at high levels in the structure.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb51ce10-e5ea-4ef6-8e72-a624f216c33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. Can hierarchical clustering be used for both numerical and categorical data? If yes, how are the distance\n",
    "metrics different for each type of data?\n",
    "It can deal with categorical variables, throughout the specification of proper dissimilarity measures.\n",
    "In particular, it can deal with numerical variables using the Euclidean distance.\n",
    "Most distance metrics, and hence the hierarchical clustering methods, work either with continuous-only\n",
    "or categorical-only data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca507e8-ce03-4191-8731-b1025d7dfb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. How can you use hierarchical clustering to identify outliers or anomalies in your data?\n",
    "First apply clustering algorithm K-Means and Hierarchical clustering on a data set then find outliers from the\n",
    "each resulting clustering. In K-Means clustering outliers are found by distance based approach and cluster based\n",
    "approach. In case of hierarchical clustering, by using dendrogram outliers are found.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de36f1f-4503-44ea-bb1e-53da77c12ed7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce13d179-41e6-4e3c-85cb-e8073811f6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is a contingency matrix, and how is it used to evaluate the performance of a classification model?\n",
    "In statistics, a contingency table (also referred to as cross tabulation or cross tab) is a type of table in a matrix\n",
    "format that displays the (multivariate) frequency distribution of the variables. It is often used to record and analyze\n",
    "the relation between two or more categorical variables\n",
    "There are many ways for measuring classification performance. Accuracy, confusion matrix, log-loss, and AUC-ROC are \n",
    "some of the most popular metrics. Precision-recall is a widely used metrics for classification problems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139f9e0f-786a-4560-a10a-7bc91dce243c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. How is a pair confusion matrix different from a regular confusion matrix, and why might it be useful in \n",
    "certain situations?\n",
    "Pair confusion matrix arising from two clusterings [1]. The pair confusion matrix computes a 2 by 2 similarity \n",
    "matrix between two clusterings by considering all pairs of samples and counting pairs that are assigned into the\n",
    "same or into different clusters under the true and predicted clusterings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa778760-1e7d-467b-af93-bc7dd04ba07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. What is an extrinsic measure in the context of natural language processing, and how is it typically used to\n",
    "evaluate the performance of language models?\n",
    "Extrinsic evaluation is the best way to evaluate the performance of a language model by embedding it in an application\n",
    "and measuring how much the application improves. It is an end-to-end evaluation where we can understand if a particular\n",
    "improvement in a component is really going to help the task at hand\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415b7830-9a37-492f-9ff4-66572ef1dbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What is an intrinsic measure in the context of machine learning, and how does it differ from an extrinsic measure?\n",
    "In an intrinsic evaluation, quality of NLP systems outputs is evaluated against pre-determined ground truth\n",
    "(reference text) whereas an extrinsic evaluation is aimed at evaluating systems outputs based on their impact\n",
    "on the performance of other NLP systems.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9c68e7-7c03-4b75-b57f-33dfbf9c21e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. What is the purpose of a confusion matrix in machine learning, and how can it be used to identify \n",
    "strengths and weaknesses of a model?\n",
    "A confusion matrix presents a table layout of the different outcomes of the prediction and results of a \n",
    "classification problem and helps visualize its outcomes. It plots a table of all the predicted and actual\n",
    "values of a classifier\n",
    "A confusion matrix is a performance evaluation tool in machine learning, representing the accuracy of a\n",
    "classification model. It displays the number of true positives, true negatives, false positives, and false negatives.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74825cd-43b1-4ceb-b134-80a87c8d3878",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. What are some common intrinsic measures used to evaluate the performance of unsupervised learning algorithms,\n",
    "and how can they be interpreted?\n",
    "Different machine learning tasks require specific evaluation metrics. Regression tasks commonly use metrics such \n",
    "as Mean Squared Error (MSE), Root Mean Squared Error (RMSE), and R² (R-Squared). In contrast, classification tasks\n",
    "use metrics like Accuracy, Confusion Matrix, Precision and Recall, F1-score, and AU-ROC.\n",
    "Examples of metrics for unsupervised learning include homogeneity, completeness, V-measure, silhouette coefficient,\n",
    "and Davies–Bouldin index.\n",
    "•\tWithin-Cluster Sum Square.\n",
    "•\tSilhouette Coefficient.\n",
    "•\tCalinski-Harabasz Index.\n",
    "•\tDavies-Bouldin Index.\n",
    "•\tCumulative Explained Variance.\n",
    "•\tTrustworthiness.\n",
    "•\tSammon's Mapping\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632e0037-c203-4c31-97ae-4afaed286f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. What are some limitations of using accuracy as a sole evaluation metric for classification tasks, and\n",
    "how can these limitations be addressed?\n",
    "•\tAccuracy is biased when there is an imbalance of data. ...\n",
    "•\tAccuracy took the false prediction as equal to each other, whether a false positive or false negative. ...\n",
    "•\tAccuracy is not a good representation for models with multiclass labels.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d5213e-1d25-42c9-8fa7-2f130faaf64d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6738a9d-0caa-463c-891c-65d6c40d6c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. Describe the decision tree classifier algorithm and how it works to make predictions.\n",
    "Decision trees is a type of supervised machine learning algorithm that is used by the\n",
    "Train Using AutoML tool and classifies or regresses the data using true or false answers\n",
    "to certain questions. The resulting structure, when visualized, is in the form of a tree\n",
    "with different types of nodes—root, internal, and leaf.\n",
    "\n",
    "\n",
    "Q2. Provide a step-by-step explanation of the mathematical intuition behind decision \n",
    "tree classification.\n",
    "First, we calculate the percentage of records that fall into each class (p). Then we square\n",
    "those numbers and add them together. Finally, we subtract that number from one.\n",
    "Because decision trees split data into more than one group, our final step is to calculate \n",
    "the weighted average of the Gini Impurity in each group\n",
    "\n",
    "\n",
    "Q3. Explain how a decision tree classifier can be used to solve a binary classification problem.\n",
    "The key idea is to use a decision tree to partition the data space into dense regions and sparse\n",
    "regions. The splitting of a binary tree can either be binary or multiway.\n",
    "The algorithm keeps on splitting the tree until the data is sufficiently homogeneous.\n",
    "\n",
    "\n",
    "Q4. Discuss the geometric intuition behind decision tree classification and \n",
    "how it can be used to make predictions.\n",
    "Every decision that we make at internal node, it will create a hyperplane – in other words,\n",
    "corresponding to every decision / splitting of the data set, we will have a hyperplane.\n",
    "All of your hyperplanes are axis-parallel – they are parallel to either X-axis or Y-axis.\n",
    "\n",
    "\n",
    "Q5. Define the confusion matrix and describe how it can be used to evaluate the performance\n",
    "of a classification model.\n",
    "Every decision that we make at internal node, it will create a hyperplane – in other words,\n",
    "corresponding to every decision / splitting of the data set, we will have a hyperplane.\n",
    "All of your hyperplanes are axis-parallel – they are parallel to either X-axis or Y-axis.\n",
    "\n",
    "\n",
    "Q6. Provide an example of a confusion matrix and explain how precision, recall,\n",
    "and F1 score can be calculated from it.\n",
    "The confusion matrix is a table that summarizes how successful the classification model \n",
    "is at predicting examples belonging to various classes. One axis of the confusion matrix\n",
    "is the label that the model predicted, and the other axis is the actual label. \n",
    "Let's take an example of classifying:\n",
    " \n",
    "In above confusion matrix table, green diagonal shows the result that in actually they\n",
    "are one thing and model also predicted that same thing but red diagonal shows the result \n",
    "that in actual they are one thing and model predicted it another thing.\n",
    "We can use confusion matrix when we compare different model by looking how well it predicted\n",
    "a true positive(TP) and true negative(TN). If one model predicted a TP and TN very well\n",
    "than other model then we choose this model as our base model.\n",
    "PRECISION\n",
    "In definition it is define as the actual correct prediction divided by total prediction made by model. \n",
    "In simple language let our model predict that out of 10 patient 7 has heart disease and among that\n",
    "predicted 7 patient, only 3 has actual heart disease so in this case precision is 3/7 = 0.428\n",
    " \n",
    "\n",
    "\n",
    "In our case true positive(TP) is 3 and false positive(FP) 7-3=4\n",
    "RECALL\n",
    "In an classification problem with two classes, recall is calculated as the number of true\n",
    "positives divided by the total number of true positives and false negatives.\n",
    "In or case of heart disease patient let's suppose there is 7 actual heart patient\n",
    "but our model predict only 5 has heart disease so in this case recall is 5/7=0.714\n",
    "How to choose Precision and Recall?\n",
    "Almost always, in practice, we have to choose between a high precision or a high recall. \n",
    "It’s usually impossible to have both. We can achieve either of the two by various means:\n",
    "•\tAssigning a higher weighting to the examples of a specific class (the SVM algorithm \n",
    "                                accepts weightings of classes as input)\n",
    "•\t Tuning hyperparameters to maximize precision or recall on the validation set.\n",
    "•\tVarying the decision threshold for algorithms that return probabilities of classes; \n",
    "for instance, if we use logistic regression or decision tree, to increase precision \n",
    "(at the cost of a lower recall), we can decide that the prediction will be positive only \n",
    "if the probability returned by the model is higher than 0.9.\n",
    "Even if precision and recall are defined for the binary classification case, you can\n",
    "always use it to assess a multiclass classification model.\n",
    "To do that, first select a class for which you want to assess these metrics.\n",
    "Then you consider all examples of the selected class as positives and all examples\n",
    "of the remaining classes as negatives.\n",
    "ACCURACY\n",
    "It is defined as total correctly classified example divided by the total number of \n",
    "classified examples. Lets express it in terms of confusion matrix:\n",
    " \n",
    "\n",
    "\n",
    "This metric is very important when error in predicting all class is equally important.\n",
    "Here False positive is most important to address than False negative.\n",
    "Lets take a example of an email is spam or not spam.\n",
    "In this case if our model classify a email send by boss is spam and don't show it \n",
    "is more harmful than showing small amount of email as spam.\n",
    "AREA UNDER THE ROC CURVE (AUC)\n",
    "It can only be used to assess classifiers that return some confidence score \n",
    "(or a probability) of prediction.\n",
    "For example, logistic regression, neural networks,\n",
    "and decision trees (and ensemble models based on decision trees)\n",
    "can be assessed using ROC curves.\n",
    "ROC curve commonly use the combination of true positive rate(TPR) \n",
    "and false positive rate(FPR) and that is given as:\n",
    " \n",
    "\n",
    "\n",
    "The higher the area under the ROC curve (AUC), the better the classifier.\n",
    "A classifier with an AUC higher than 0.5 is better than a random classifier.\n",
    "If AUC is lower than 0.5, then something is wrong with your model. \n",
    "A perfect classifier would have an AUC of 1.\n",
    " \n",
    "ROC curve capture more than one aspect of the classification\n",
    "(by taking both false positives and negatives into account)\n",
    "and allow visually and with low effort comparing the performance of different models.\n",
    "F1 SCORE\n",
    "F1 score is a weighted average of precision and recall.\n",
    "As we know in precision and in recall there is false positive and\n",
    "false negative so it also consider both of them. \n",
    "F1 score is usually more useful than accuracy, especially\n",
    "if you have an uneven class distribution. \n",
    "Accuracy works best if false positives and false negatives have similar cost.\n",
    "If the cost of false positives and false negatives are very different, \n",
    "it’s better to look at both Precision and Recall.\n",
    "F1 Score = 2*(Recall * Precision) / (Recall + Precision)\n",
    "\n",
    "\n",
    "Q7. Discuss the importance of choosing an appropriate evaluation metric for a\n",
    "classification problem and explain how this can be done.\n",
    "\n",
    "In order to accurately assess the performance of a classifier and to make \n",
    "informed decisions based on its predictions, it is crucial to choose an \n",
    "appropriate evaluation metric. In most situations, this choice will highly \n",
    "depend on the specific problem at hand.\n",
    "\n",
    "\n",
    "Q8. Provide an example of a classification problem where precision is\n",
    "the most important metric, and explain why.\n",
    "While searching something on the web, the model does not care about something\n",
    "irrelevant and not retrieved (this is the true negative case). \n",
    "Therefore only TP, FP, FN are used in Precision.\n",
    "Out of all the positive predicted, what percentage is truly positive.\n",
    " \n",
    "The precision value lies between 0 and 1.\n",
    "Example: Spam detection.\n",
    " \n",
    "In the detection of spam mail, it is okay if any spam mail remains undetected \n",
    "(false negative), but what if we miss any critical mail because it is classified \n",
    "as spam (false positive). In this situation, False Positive should be as low as\n",
    "possible. Here, precision is more vital as compared to recall.\n",
    "When comparing different models, it will be difficult to decide which is better\n",
    "(high precision and low recall or vice-versa). Therefore, there should be a \n",
    "metric that combines both of these. One such metric is the F1 score\n",
    "\n",
    "\n",
    "\n",
    "Q9. Provide an example of a classification problem where recall is the most important\n",
    "metric, and explain why.\n",
    "Out of the total positive, what percentage are predicted positive. \n",
    "It is the same as TPR (true positive rate).\n",
    " \n",
    " \n",
    "Confusion Matrix for Credit Card Fraud Detection\n",
    "We do not want to miss any fraud transactions. Therefore, we want \n",
    "False-Negative to be as low as possible. In these situations,\n",
    "we can compromise with the low precision, but recall should be high. \n",
    "Similarly, in the medical application, we don’t want to miss any patient. \n",
    "Therefore we focus on having a high recall\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

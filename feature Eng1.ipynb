{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c203b138-7a80-485e-9cd4-cf700d7ace59",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some\n",
    "algorithms that are not affected by missing values.\n",
    "\n",
    "Missing data is defined as the values or data that is not stored (or not present) for some variable/s in\n",
    "the given dataset. Below is a sample of the missing data from the Titanic dataset. You can see the columns \n",
    "'Age' and 'Cabin' have some missing values.\n",
    "\n",
    "It is important to handle missing values as they can lead to inaccurate conclusions about the data, which\n",
    "can significantly impact the accuracy of the analysis. There are several methods available to handle missing \n",
    "values, such as removal, imputation, flagging, etc.\n",
    "\n",
    "The k-NN algorithm can ignore a column from a distance measure when a value is missing. Naive Bayes can also\n",
    "support missing values when making a prediction. These algorithms can be used when the dataset contains null \n",
    "or missing values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3178f1-b669-4388-8472-635a96f895e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2: List down techniques used to handle missing data. Give an example of each with python code.\n",
    "The techniques to handle missing data are:\n",
    "•\tDeleting Rows with missing values.\n",
    "•\tImpute missing values for continuous variable.\n",
    "•\tImpute missing values for categorical variable.\n",
    "•\tOther Imputation Methods.\n",
    "•\tUsing Algorithms that support missing values.\n",
    "•\tPrediction of missing values.\n",
    "An example of each with python code.\n",
    "\n",
    "1. Delete Rows with Missing Values\n",
    "One way of handling missing values is the deletion of the rows or columns having null values.\n",
    "If any columns have more than half of the values as null then you can drop the entire column.\n",
    "In the same way, rows can also be dropped if having one or more columns values as null. Before \n",
    "using this method one thing we have to keep in mind is that we should not be losing information.\n",
    "Because if the information we are deleting is contributing to the output value then we should not \n",
    "use this method because this will affect our output.\n",
    "When to delete the rows/column in a dataset?\n",
    "•\tIf a certain column has many missing values then you can choose to drop the entire column.\n",
    "•\tWhen you have a huge dataset. Deleting for e.g. 2-3 rows/columns will not make much difference.\n",
    "•\tOutput results do not depend on the Deleted data. \n",
    "Note: No doubt it is one of the quick techniques one can use to deal with missing values.\n",
    "But this approach is not recommended. \n",
    "\n",
    "2. Replacing With Arbitrary Value\n",
    "If you can replace the missing value with some arbitrary value using fillna().\n",
    "Ex. In the below code, we are replacing the missing values with ‘0’.As well you can replace\n",
    "any particular column missing values with some arbitrary value also.\n",
    "•\tReplacing with previous value – Forward fill\n",
    "We can impute the values with the previous value by using forward fill. It is mostly used in time series data.\n",
    "Syntax: df.fillna(method=’ffill’)\n",
    " \n",
    "•\tReplacing with next value – Backward fill\n",
    "In backward fill, the missing value is imputed using the next value. It is mostly used in time series data.\n",
    " \n",
    "3. Interpolation\n",
    "Missing values can also be imputed using ‘interpolation’. Pandas interpolate method can be used to replace \n",
    "the missing values with different interpolation methods like ‘polynomial’, ‘linear’, ‘quadratic’.\n",
    "The default method is ‘linear’.\n",
    "Syntax: df.interpolate(method=’linear’)\n",
    "For the time-series dataset variable, it makes sense to use the interpolation of the variable before \n",
    "and after a timestamp for a missing value. Interpolation in most cases supposed to be the best technique \n",
    "to fill missing values.\n",
    "Handling missing values: python code:\n",
    "We have taken dataset titanic.csv which is freely available at kaggle.com.This dataset was taken as it has missing values.\n",
    "\n",
    "1.Reading the data\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"train.csv\", usecols=['Age','Fare','Survived'])\n",
    "df\n",
    " \n",
    "The dataset is read and used three columns ‘Age’, ’Fare’, ’Survived’.\n",
    " \n",
    "2. Checking if there are missing values\n",
    "df.isnull().sum()\n",
    " \n",
    "Output:\n",
    "Survived      4\n",
    "Age         179\n",
    "Fare          2\n",
    "dtype: int64\n",
    " \n",
    "3.Filling missing values with 0\n",
    "new_df = df.fillna(0)\n",
    "new_df\n",
    " \n",
    " \n",
    " \n",
    "4. Filling NaN values with forward fill value\n",
    "new_df = df.fillna(method=\"ffill\")\n",
    "new_df\n",
    "If we use forward fill that simply means we are forwarding the previous value where ever we have NaN values.\n",
    "5. Setting forward fill limit to 1 \n",
    "new_df = df.fillna(method=\"ffill\",limit=1)\n",
    "new_df\n",
    " \n",
    "Now we have set the limit of forward fill to 1 which means that only once, the value will be copied below.\n",
    "Like in this case we had three NaN values consecutively in column Survived. But one NaN value was filled\n",
    "only as the limit is set to 1.\n",
    "6. Filling NaN values in Backward Direction\n",
    "new_df = df.fillna(method=\"bfill\")new_df    \n",
    " \n",
    "7. Interpolate of missing values\n",
    "new_df = df.interpolate() \n",
    "df    \n",
    " \n",
    "In this, we were having two values 22 and 26. And in between value was a NaN value. So that NaN value is\n",
    "computed by getting the mean of 22 and 26 i.e. 24. In the same way, other NaN values were also computed.\n",
    "8. Dropna()\n",
    "new_df = df.dropna()\n",
    "new_df     \n",
    " \n",
    "Previously we were having 891 rows and after running this code we are left with 710 rows because some of the \n",
    "rows were continuing NaN values were dropped.\n",
    "9. Deleting the rows having all NaN values\n",
    "new_df = df.dropna(how='all')\n",
    "new_df      \n",
    "Those rows in which all the values are NaN values will be deleted. If the row even has one value even then\n",
    "it will not be dropped \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c72a84-0551-41ce-b6b5-26ef1d693a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?\n",
    "\n",
    "A classification data set with skewed class proportions is called imbalanced. Classes that make up a large\n",
    "proportion of the data set are called majority classes. Those that make up a smaller proportion are minority classes.\n",
    "When a dataset is imbalanced, several issues may arise. Models may exhibit bias toward the majority class,\n",
    "resulting in poor predictions for the minority class. Accuracy as an evaluation metric can be misleading, \n",
    "as it may appear high while the model's performance on the minority class is lacking\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a58599-f8ea-42e2-84a0-e7e1f95aac72",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down-\n",
    "sampling are required.\n",
    "\n",
    "Downsampling. The idea of downsampling is remove samples from the signal, whilst maintaining its length with \n",
    "respect to time. For example, a time signal of 10 seconds length, with a sample rate of 1024Hz or samples per\n",
    "second will have 10 x 1024 or 10240 samples.\n",
    "\n",
    "df_minority=df[df['target']==1]\n",
    "df_majority=df[df['target']==0]\n",
    "\n",
    "from sklearn.utils import resample\n",
    "df_majority_downsampled=resample(df_majority,replace=False, #Sample With replacement\n",
    "         n_samples=len(df_minority),\n",
    "         random_state=42\n",
    "        )\n",
    "df_majority_downsampled.shape\n",
    "df_downsampled=pd.concat([df_minority,df_majority_downsampled])\n",
    "df_downsampled['target'].value_counts()\n",
    "\n",
    "Upsampling is the process of inserting zero-valued samples between original samples to increase the \n",
    "sampling rate. (This is sometimes called “zero-stuffing”.) This kind of upsampling adds undesired \n",
    "spectral images to the original signal, which are centered on multiples of the original sampling rate\n",
    "\n",
    "df_minority=df[df['target']==1]\n",
    "df_majority=df[df['target']==0]\n",
    "\n",
    "from sklearn.utils import resample\n",
    "df_minority_upsampled=resample(df_minority,replace=True, #Sample With replacement\n",
    "         n_samples=len(df_majority),\n",
    "         random_state=42\n",
    "        )\n",
    "\n",
    "df_minority_upsampled.shape\n",
    "df_minority_upsampled.head()\n",
    "df_upsampled=pd.concat([df_majority,df_minority_upsampled])\n",
    "df_upsampled['target'].value_counts()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3968f226-bd3a-423a-b183-9f4aeeb55fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5: What is data Augmentation? Explain SMOTE.\n",
    "Data augmentation is a technique in machine learning used to reduce overfitting when training a machine\n",
    "learning model, by training models on several slightly-modified copies of existing data.\n",
    "\n",
    "SMOTE (Synthetic Minority Over-sampling Technique) is a technique used in machine learning to address \n",
    "imbalanced datasets where the minority class has significantly fewer instances than the majority class.\n",
    "SMOTE involves generating synthetic instances of the minority class by interpolating between existing instances.\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "oversample=SMOTE()\n",
    "X,y=oversample.fit_resample(final_df[['f1','f2']],final_df['target'])\n",
    "X.shape\n",
    "y.shape\n",
    "len(y[y==0])\n",
    "len(y[y==1])\n",
    "df1=pd.DataFrame(X,columns=['f1','f2'])\n",
    "df2=pd.DataFrame(y,columns=['target'])\n",
    "oversample_df=pd.concat([df1,df2],axis=1)\n",
    "plt.scatter(oversample_df['f1'],oversample_df['f2'],c=oversample_df['target'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c6f814-176e-4e44-9956-32e0bdf0751b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6: What are outliers in a dataset? Why is it essential to handle outliers?\n",
    "An outlier in statistics is an observation that lies an abnormal distance from other values in a random sample\n",
    "from a population. There is, of course, a degree of ambiguity. Qualifying a data point as an anomaly leaves it\n",
    "up to the analyst or model to determine what is abnormal—and what to do with such data points.\n",
    "Outliers are important because they can have a large influence on statistics derived from the dataset. \n",
    "For example, the mean intake of energy or some nutrient may be [glossary term:] skewed upward or downward \n",
    "by one or a few extreme values (Learn More about Normal Distributions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18264db3-d60b-4ad0-b47f-478ff38c3446",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7: You are working on a project that requires analyzing customer data. However, you notice that some \n",
    "of the data is missing. What are some techniques you can use to handle the missing data in your analysis?\n",
    "\n",
    "\n",
    "When dealing with missing data, data scientists can use two primary methods to solve the error: imputation\n",
    "or the removal of data. The imputation method develops reasonable guesses for missing data. \n",
    "It's most useful when the percentage of missing data is low.\n",
    "\n",
    "1.Missing data can be dealt with in a variety of ways. ...\n",
    "2.Another common strategy among those who pay attention is imputation. ...\n",
    "3.Mean imputation. ...\n",
    "4.Substitution. ...\n",
    "5.Hot deck imputation. ...\n",
    "6.Cold deck imputation. ...\n",
    "7.Regression imputation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add42c72-3b2f-4fbb-8647-bdf4c33a5e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8: You are working with a large dataset and find that a small percentage of the data is missing. \n",
    "What are some strategies you can use to determine if the missing data is missing at random or if\n",
    "there is a pattern to the missing data?\n",
    "Type of missing data\t                                          Imputation method\n",
    "Missing Completely At Random\t                         Mean, Median, Mode, or any other imputation method\n",
    "Missing At Random\t                                     Multiple imputation, Regression imputation\n",
    "Missing Not At Random\t                                 Pattern Substitution, Maximum Likelihood estima\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd98e2a-820f-43f8-87ca-ca11be91bc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the\n",
    "dataset do not have the condition of interest, while a small percentage do. What are some strategies you \n",
    "can use to evaluate the performance of your machine learning model on this imbalanced dataset?\n",
    "\n",
    "This technique is used to upsample or downsample the minority or majority class. When we are using an \n",
    "imbalanced dataset, we can oversample the minority class using replacement. This technique is called \n",
    "oversampling. Similarly, we can randomly delete rows from the majority class to match them with the \n",
    "minority class which is called undersampling. After sampling the data we can get a balanced dataset \n",
    "for both majority and minority classes. So, when both classes have a similar number of records present \n",
    "in the dataset, we can assume that the classifier will give equal importance to both classes.\n",
    "An example of this technique using the sklearn library’s resample() is shown below for illustration \n",
    "purposes. Here, Is_Lead is our target variable. Let’s see the distribution of the classes in the target.\n",
    "\n",
    "\n",
    "An example of this technique using the sklearn library’s resample() is shown below for illustration\n",
    "purposes. Here, Is_Lead is our target variable. Let’s see the distribution of the classes in the target.\n",
    " \n",
    "It has been observed that our target class has an imbalance. So, we’ll try to upsample the data so \n",
    "that the minority class matches with the majority class.\n",
    "\n",
    "from sklearn.utils import resample\n",
    "#create two different dataframe of majority and minority class \n",
    "df_majority = df_train[(df_train['Is_Lead']==0)] \n",
    "df_minority = df_train[(df_train['Is_Lead']==1)] \n",
    "# upsample minority class\n",
    "df_minority_upsampled = resample(df_minority, \n",
    "                                 replace=True,    # sample with replacement\n",
    "                                 n_samples= 131177, # to match majority class\n",
    "                                 random_state=42)  # reproducible results\n",
    "# Combine majority class with upsampled minority class\n",
    "df_upsampled = pd.concat([df_minority_upsampled, df_majority])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b90653-1075-438c-a36a-6a83e48c666f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is\n",
    "unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to\n",
    "balance the dataset and down-sample the majority class?\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "np.random.seed(123)\n",
    "\n",
    "# Create a dataframe with two classes\n",
    "n_samples = 1000\n",
    "class_0_ratio = 0.9\n",
    "n_class_0 = int(n_samples * class_0_ratio)\n",
    "n_class_1 = n_samples - n_class_0\n",
    "n_class_0,n_class_1\n",
    "\n",
    "## CREATE MY DATAFRAME WITH IMBALANCED DATASET\n",
    "class_0 = pd.DataFrame({\n",
    "    'feature_1': np.random.normal(loc=0, scale=1, size=n_class_0),\n",
    "    'feature_2': np.random.normal(loc=0, scale=1, size=n_class_0),\n",
    "    'target': [0] * n_class_0\n",
    "})\n",
    "\n",
    "class_1 = pd.DataFrame({\n",
    "    'feature_1': np.random.normal(loc=2, scale=1, size=n_class_1),\n",
    "    'feature_2': np.random.normal(loc=2, scale=1, size=n_class_1),\n",
    "    'target': [1] * n_class_1\n",
    "})\n",
    "df=pd.concat([class_0,class_1]).reset_index(drop=True)\n",
    "df.tail()\n",
    "df['target'].value_counts()\n",
    "## upsampling\n",
    "df_minority=df[df['target']==1]\n",
    "df_majority=df[df['target']==0]\n",
    "from sklearn.utils import resample\n",
    "df_minority_upsampled=resample(df_minority,replace=True, #Sample With replacement\n",
    "         n_samples=len(df_majority),\n",
    "         random_state=42\n",
    "        )\n",
    "df_minority_upsampled.shape\n",
    "df_minority_upsampled.head()\n",
    "df_upsampled=pd.concat([df_majority,df_minority_upsampled])\n",
    "df_upsampled['target'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67abe88b-ad8d-4aa5-9aa8-266ff89a35ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while\n",
    "working on a project that requires you to estimate the occurrence of a rare event. \n",
    "What methods can you employ to balance the dataset and up-sample the minority class?\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "np.random.seed(123)\n",
    "\n",
    "# Create a dataframe with two classes\n",
    "n_samples = 1000\n",
    "class_0_ratio = 0.9\n",
    "n_class_0 = int(n_samples * class_0_ratio)\n",
    "n_class_1 = n_samples - n_class_0\n",
    "\n",
    "class_0 = pd.DataFrame({\n",
    "    'feature_1': np.random.normal(loc=0, scale=1, size=n_class_0),\n",
    "    'feature_2': np.random.normal(loc=0, scale=1, size=n_class_0),\n",
    "    'target': [0] * n_class_0\n",
    "})\n",
    "\n",
    "class_1 = pd.DataFrame({\n",
    "    'feature_1': np.random.normal(loc=2, scale=1, size=n_class_1),\n",
    "    'feature_2': np.random.normal(loc=2, scale=1, size=n_class_1),\n",
    "    'target': [1] * n_class_1\n",
    "})\n",
    "\n",
    "df = pd.concat([class_0, class_1]).reset_index(drop=True)\n",
    "\n",
    "# Check the class distribution\n",
    "print(df['target'].value_counts())\n",
    "## downsampling\n",
    "df_minority=df[df['target']==1]\n",
    "df_majority=df[df['target']==0]\n",
    "from sklearn.utils import resample\n",
    "df_majority_downsampled=resample(df_majority,replace=False, #Sample With replacement\n",
    "         n_samples=len(df_minority),\n",
    "         random_state=42\n",
    "        )\n",
    "df_majority_downsampled.shape\n",
    "df_downsampled=pd.concat([df_minority,df_majority_downsampled])\n",
    "df_downsampled['target'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e06790e-306d-4ad4-8f2b-2ae4e2bc2c08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

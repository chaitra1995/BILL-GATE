{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3713aa-7c23-49c7-8022-f88460dbe90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1: Define overfitting and underfitting in machine learning. What are the consequences of each,\n",
    "and how can they be mitigated? \n",
    "\n",
    "Overfitting:\n",
    "Overfitting occurs when our machine learning model tries to cover all the data points or more than \n",
    "the required data points present in the given dataset. Because of this, the model starts caching noise\n",
    "and inaccurate values present in the dataset, and all these factors reduce the efficiency and accuracy \n",
    "of the model. The overfitted model has low bias and high variance.\n",
    "\n",
    "Underfitting:\n",
    "Underfitting occurs when a model fails to capture the underlying patterns and relationships within the data.\n",
    "In other words, it is an oversimplified representation that struggles to generalize to new instances. \n",
    "Imagine an underconfident student who lacks a deep understanding of the subject matter.\n",
    "They might struggle to apply their knowledge to different problems.\n",
    "An underfitted model has high bias and low variance.\n",
    "\n",
    "Like overfitting, when a model is underfitted, it cannot establish the dominant trend within the data,\n",
    "resulting in training errors and poor performance of the model. If a model cannot generalize well to new data,\n",
    "then it cannot be leveraged for classification or prediction tasks.\n",
    "\n",
    "When data scientists use machine learning models for making predictions, they first train the model on a known\n",
    "data set. Then, based on this information, the model tries to predict outcomes for new data sets. An overfit \n",
    "model can give inaccurate predictions and cannot perform well for all types of new data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb6794c-7a63-415e-b0f8-8306df72b17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2: How can we reduce overfitting? Explain in brief.\n",
    "We can reduce overfitting by diversifying and scaling your training data set or using some other \n",
    "data science strategies, like those given below.\n",
    "\n",
    "a.Early stopping\n",
    "Early stopping pauses the training phase before the machine learning model learns the noise in the data.\n",
    "However, getting the timing right is important; else the model will still not give accurate results.\n",
    "\n",
    "b.Pruning\n",
    "You might identify several features or parameters that impact the final prediction when you build a model.\n",
    "Feature selection—or pruning—identifies the most important features within the training set and eliminates\n",
    "irrelevant ones. For example, to predict if an image is an animal or human, you can look at various input\n",
    "parameters like face shape, ear position, body structure, etc. You may prioritize face shape and ignore \n",
    "the shape of the eyes.\n",
    "\n",
    "c.Regularization\n",
    "Regularization is a collection of training/optimization techniques that seek to reduce overfitting. \n",
    "These methods try to eliminate those factors that do not impact the prediction outcomes by grading \n",
    "features based on importance. For example, mathematical calculations apply a penalty value to features \n",
    "with minimal impact. Consider a statistical model attempting to predict the housing prices of a city in\n",
    "20 years. Regularization would give a lower penalty value to features like population growth and average\n",
    "annual income but a higher penalty value to the average annual temperature of the city.\n",
    "\n",
    "d.Ensembling\n",
    "Ensembling combines predictions from several separate machine learning algorithms. Some models are called\n",
    "weak learners because their results are often inaccurate. Ensemble methods combine all the weak learners\n",
    "to get more accurate results. They use multiple models to analyze sample data and pick the most accurate\n",
    "outcomes. The two main ensemble methods are bagging and boosting. Boosting trains different machine \n",
    "learning models one after another to get the final result, while bagging trains them in parallel.\n",
    "\n",
    "e.Data augmentation\n",
    "Data augmentation is a machine learning technique that changes the sample data slightly every time\n",
    "the model processes it. You can do this by changing the input data in small ways. When done in moderation,\n",
    "data augmentation makes the training sets appear unique to the model and prevents the model from learning\n",
    "their characteristics. For example, applying transformations such as translation, flipping, and rotation\n",
    "to input images.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf99614-63a5-4f1d-8f19-2589cc5dbebc",
   "metadata": {},
   "outputs": [],
   "source": [
    " Q3: Explain underfitting. List scenarios where underfitting can occur in ML.\n",
    "\n",
    "Underfitting occurs when a model fails to capture the underlying patterns and relationships within the data.\n",
    "In other words, it is an oversimplified representation that struggles to generalize to new instances.\n",
    "Imagine an underconfident student who lacks a deep understanding of the subject matter. They might \n",
    "struggle to apply their knowledge to different problems.\n",
    "An underfitted model has high bias and low variance.\n",
    "Scenarios:\n",
    "Overfitting happens when a model learns the detail and noise in the training data to the extent that it \n",
    "negatively impacts the performance of the model on new data. This means that the noise or random fluctuations \n",
    "in the training data is picked up and learned as concepts by the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66181e8-028b-43e3-ba9d-bd60a2b95554",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and variance,\n",
    "and how do they affect model performance?\n",
    "The bias-variance tradeoff is a fundamental concept in machine learning and statistics. It refers to the\n",
    "delicate balance between two sources of error in a predictive model: bias and variance.Bias represents \n",
    "the error due to overly simplistic assumptions in the learning algorithm. High bias can cause the model \n",
    "to underfit the data, leading to poor performance on both training and unseen data.Variance, on the other\n",
    "hand, reflects the model’s sensitivity to small fluctuations in the training data. High variance can lead\n",
    "to overfitting, where the model captures noise in the training data and performs poorly on new, unseen data.\n",
    "The goal is to find the right level of complexity in a model to minimize both bias and variance, achieving \n",
    "good generalization to new data. Balancing these factors is essential for building models that perform \n",
    "well on a variety of datasets.\n",
    "\n",
    "Understand Bias-Variance Tradeoff with the help of an example\n",
    "How do we relate the above concepts to our Knn model from earlier? Let’s find out!\n",
    "In our model, say, for, k = 1, the point closest to the datapoint in question will be considered.\n",
    "Here, the prediction might be accurate for that particular data point so the bias error will be less.\n",
    "However, the variance error will be high since only the one nearest point is considered and this doesn’t\n",
    "take into account the other possible points. What scenario do you think this corresponds to? Yes, you are\n",
    "thinking right, this means that our model is overfitting. On the other hand, for higher values of k, many\n",
    "more points closer to the datapoint in question will be considered. This would result in higher bias error\n",
    "and underfitting since many points closer to the datapoint are considered and thus it can’t learn the\n",
    "specifics from the training set. However, we can account for a lower variance error for the testing set\n",
    "which has unknown values.To achieve a balance between the Bias error and the Variance error, we need\n",
    "a value of k such that the model neither learns from the noise (overfit on data) nor makes sweeping\n",
    "assumptions on the data(underfit on data). To keep it simpler, a balanced model would look like this:\n",
    " \n",
    "Though some points are classified incorrectly, the model generally fits most of the datapoints accurately. \n",
    "The balance between the Bias error and the Variance error is the Bias-Variance Tradeoff.\n",
    "The following bulls-eye diagram explains the tradeoff better:\n",
    " \n",
    "The center i.e. the bull’s eye is the model result we want to achieve that perfectly predicts all the\n",
    "values correctly. As we move away from the bull’s eye, our model starts to make more and more wrong\n",
    "predictions.A model with low bias and high variance predicts points that are around the center generally,\n",
    "but pretty far away from each other. A model with high bias and low variance is pretty far away from the\n",
    "bull’s eye, but since the variance is low, the predicted points are closer to each other.In terms of model\n",
    "complexity, we can use the following diagram to decide on the optimal complexity of our model.\n",
    " \n",
    "So, what do you think is the optimum value for k?\n",
    "From the above explanation, we can conclude that the k for which\n",
    "•\tthe testing score is the highest, and\n",
    "•\tboth the test score and the training score are close to each other\n",
    "is the optimal value of k. So, even though we are compromising on a lower training score, we still\n",
    "get a high score for our testing data which is more crucial – the test data is after all unknown data.\n",
    "Let us make a table for different values of k to further prove this:\n",
    " \n",
    "\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf6bad1-62c9-4497-8aa0-4298ce37dcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models.\n",
    "How can you determine whether your model is overfitting or underfitting?\n",
    "\n",
    "The cause for overfitting is a misinterpretation of training data. So, the model produces less accurate\n",
    "results for unseen data. However, an overfitted model generates very high accuracy scores during the\n",
    "training phase.\n",
    "Similarly, underfitted models don’t effectively capture the relationship between the input and output\n",
    "data because it is too simple. As a result, the underfitted model performs poorly, even with the training data.\n",
    "If the data scientist is not careful, they can easily be mistaken and deploy an overfitted model into\n",
    "production. Though, applying the decisions of an overfitted model’s predictions will result in errors.\n",
    "For example, the business may lose value or come face to face with dissatisfied customers.\n",
    "Deploying an underfitting model into production may hurt business as it generates inaccurate results.\n",
    "Thus, shaping decisions upon erroneous outputs leads to unreliable business decisions, as well.\n",
    "We need to closely watch model loss and accuracy to decide how the model is fitted to the dataset:\n",
    "\n",
    "3.1. Detecting Overfitting\n",
    "We can detect overfitting in different steps in the machine learning life cycle using various techniques.\n",
    "Adopting the holdout method and saving a portion of the dataset for testing is crucial.\n",
    "To determine when overfitting begins, we plot training error and validation error together.\n",
    "As we train the model, we expect both to decrease at the beginning. However, after some point,\n",
    "the validation error would increase, whereas the training error keeps dropping. Training further\n",
    "after this point leads to overfitting:\n",
    "\n",
    "3.2. Detecting Underfitting\n",
    "Usually, detecting underfitting is more straightforward than detecting overfitting.\n",
    "Even without using a test set, we can decide if the model is performing poorly on the training\n",
    "set or not. If the model accuracy is insufficient on the training data, it has high bias and hence, underfitting.\n",
    "A challenge in machine learning is to decide the model complexity as we don’t know the underlying \n",
    "optimal complexity of the dataset. Moreover, we have incomplete information regarding the environment,\n",
    "and the data we have contains noise.\n",
    "Under these circumstances, we need to overcome the bias-variance tradeoff to fit a model:\n",
    "\n",
    "4. How to Cure Underfitting and Overfitting\n",
    "We have already discussed what overfitting and underfitting in machine learning are and how we can detect \n",
    "them. Now, let’s try to understand what are our options to combat them:\n",
    "4.1. Cures for Overfitting\n",
    "If the model is overfitting, the most sensible approach is to reduce model complexity. \n",
    "This way, we can enable it to generalize better.\n",
    "In an attempt to reduce model flexibility, we can reduce the number of input features. \n",
    "To enable fewer feature combinations within the model, we can either manually select which\n",
    "features to keep or take advantage of a feature selection algorithm.\n",
    "Alternatively, we can apply regularization to suppress higher-order terms in our model.\n",
    "So, we keep all the features but limit the magnitude of their feature importance in the outcome.\n",
    "As expected, regularization works well with a lot of features that contain slightly helpful information.\n",
    "For instance, adding a dropout layer in neural networks is a form of regularization. Moreover,\n",
    "we can set a regularization term to limit the weights of a neural network.\n",
    "Early stopping is another type of regularization we can utilize for iterative models.\n",
    "The main idea is to stop model training when specific criteria are satisfied, such as \n",
    "when validation accuracy starts to decrease.\n",
    "Another possible approach to model fitting is to use more training examples to train the model to generalize better.\n",
    "4.2. Cures for Underfitting\n",
    "To prevent underfitting, we need to ensure the model complexity.\n",
    " \n",
    "The first method that comes to mind is to obtain more training data. However, this is not an easy task for\n",
    "most problems. In such cases, we can bring data augmentation into service. So, we can increase the amount\n",
    "of data available by creating slightly modified synthetic copies of the data points at hand.\n",
    "Similarly, increasing the number of passes on the training data is a viable approach for iterative algorithms.\n",
    "Increasing the number of epochs in a neural network is a well-known practice to ensure model fitting.\n",
    "Another way to increase model complexity is to increase the size and number of model parameters.\n",
    "We can introduce engineered features from the dataset. For example, a product of numerical features\n",
    "or n parameter of n-grams generates new features.\n",
    "Alternatively, we can reduce regularization. Some implementations implicitly include default regularization\n",
    "parameters to overfitting. Checking the default parameters is a good start point. As we’re trying to get out\n",
    "of a limited feature set, there’s no need to introduce limiting terms into the model.\n",
    "Replacing the approach is another solution. For example, the selection of the kernel function in SVM \n",
    "determines the model complexity. Thus, the choice of kernel function might lead to overfitting or underfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099fe612-702c-462e-b1cc-f075b3d5a8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models. How can you determine whether your model is overfitting or underfitting?\n",
    "\n",
    "The cause for overfitting is a misinterpretation of training data. So, the model produces less accurate results for unseen data. However, an overfitted model generates very high accuracy scores during the training phase.\n",
    "Similarly, underfitted models don’t effectively capture the relationship between the input and output data because it is too simple. As a result, the underfitted model performs poorly, even with the training data.\n",
    "If the data scientist is not careful, they can easily be mistaken and deploy an overfitted model into production. Though, applying the decisions of an overfitted model’s predictions will result in errors. For example, the business may lose value or come face to face with dissatisfied customers.\n",
    "Deploying an underfitting model into production may hurt business as it generates inaccurate results. Thus, shaping decisions upon erroneous outputs leads to unreliable business decisions, as well.\n",
    "We need to closely watch model loss and accuracy to decide how the model is fitted to the dataset:\n",
    "\n",
    " \n",
    "\n",
    "3.1. Detecting Overfitting\n",
    "We can detect overfitting in different steps in the machine learning life cycle using various techniques. Adopting the holdout method and saving a portion of the dataset for testing is crucial.\n",
    "To determine when overfitting begins, we plot training error and validation error together. As we train the model, we expect both to decrease at the beginning. However, after some point, the validation error would increase, whereas the training error keeps dropping. Training further after this point leads to overfitting:\n",
    " \n",
    "3.2. Detecting Underfitting\n",
    "Usually, detecting underfitting is more straightforward than detecting overfitting. Even without using a test set, we can decide if the model is performing poorly on the training set or not. If the model accuracy is insufficient on the training data, it has high bias and hence, underfitting.\n",
    "A challenge in machine learning is to decide the model complexity as we don’t know the underlying optimal complexity of the dataset. Moreover, we have incomplete information regarding the environment, and the data we have contains noise.\n",
    "Under these circumstances, we need to overcome the bias-variance tradeoff to fit a model:\n",
    "\n",
    "\n",
    " \n",
    " \n",
    "4. How to Cure Underfitting and Overfitting\n",
    "We have already discussed what overfitting and underfitting in machine learning are and how we can detect them. Now, let’s try to understand what are our options to combat them:\n",
    "4.1. Cures for Overfitting\n",
    "If the model is overfitting, the most sensible approach is to reduce model complexity. This way, we can enable it to generalize better.\n",
    "In an attempt to reduce model flexibility, we can reduce the number of input features. To enable fewer feature combinations within the model, we can either manually select which features to keep or take advantage of a feature selection algorithm.\n",
    "Alternatively, we can apply regularization to suppress higher-order terms in our model. So, we keep all the features but limit the magnitude of their feature importance in the outcome. As expected, regularization works well with a lot of features that contain slightly helpful information.\n",
    "For instance, adding a dropout layer in neural networks is a form of regularization. Moreover, we can set a regularization term to limit the weights of a neural network.\n",
    "Early stopping is another type of regularization we can utilize for iterative models. The main idea is to stop model training when specific criteria are satisfied, such as when validation accuracy starts to decrease.\n",
    "Another possible approach to model fitting is to use more training examples to train the model to generalize better.\n",
    "4.2. Cures for Underfitting\n",
    "To prevent underfitting, we need to ensure the model complexity.\n",
    " \n",
    "The first method that comes to mind is to obtain more training data. However, this is not an easy task for most problems. In such cases, we can bring data augmentation into service. So, we can increase the amount of data available by creating slightly modified synthetic copies of the data points at hand.\n",
    "Similarly, increasing the number of passes on the training data is a viable approach for iterative algorithms. Increasing the number of epochs in a neural network is a well-known practice to ensure model fitting.\n",
    "Another way to increase model complexity is to increase the size and number of model parameters. We can introduce engineered features from the dataset. For example, a product of numerical features or n parameter of n-grams generates new features.\n",
    "Alternatively, we can reduce regularization. Some implementations implicitly include default regularization parameters to overfitting. Checking the default parameters is a good start point. As we’re trying to get out of a limited feature set, there’s no need to introduce limiting terms into the model.\n",
    "Replacing the approach is another solution. For example, the selection of the kernel function in SVM determines the model complexity. Thus, the choice of kernel function might lead to overfitting or underfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d249e578-93d5-4a25-a098-fd355971a5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    " Q6: Compare and contrast bias and variance in machine learning. What are some examples of high\n",
    "    bias and high variance models, and how do they differ in terms of their performance? \n",
    "In general, a machine learning model analyses the data, find patterns in it and make predictions.\n",
    "While training, the model learns these patterns in the dataset and applies them to test data for\n",
    "prediction. While making predictions, a difference occurs between prediction values made by the\n",
    "model and actual values/expected values, and this difference is known as bias errors or Errors due to\n",
    "bias. It can be defined as an inability of machine learning algorithms such as Linear Regression to\n",
    "capture the true relationship between the data points. Each algorithm begins with some amount of bias\n",
    "because bias occurs from assumptions in the model, which makes the target function simple to learn.\n",
    "A model has either:\n",
    "o\tLow Bias: A low bias model will make fewer assumptions about the form of the target function.\n",
    "o\tHigh Bias: A model with a high bias makes more assumptions, and the model becomes unable to capture\n",
    "the important features of our dataset. A high bias model also cannot perform well on new data.\n",
    "Generally, a linear algorithm has a high bias, as it makes them learn fast. The simpler the algorithm, \n",
    "the higher the bias it has likely to be introduced. Whereas a nonlinear algorithm often has low bias.\n",
    "Some examples of machine learning algorithms with low bias are Decision Trees, k-Nearest Neighbours and\n",
    "Support Vector Machines. At the same time, an algorithm with high bias is Linear Regression, Linear\n",
    "Discriminant Analysis and Logistic Regression.\n",
    "Ways to reduce High Bias:\n",
    "High bias mainly occurs due to a much simple model. Below are some ways to reduce the high bias:\n",
    "o\tIncrease the input features as the model is underfitted.\n",
    "o\tDecrease the regularization term.\n",
    "o\tUse more complex models, such as including some polynomial features.\n",
    "What is a Variance Error?\n",
    "The variance would specify the amount of variation in the prediction if the different training data was used.\n",
    "In simple words, variance tells that how much a random variable is different from its expected value. Ideally,\n",
    "a model should not vary too much from one training dataset to another, which means the algorithm should be good \n",
    "in understanding the hidden mapping between inputs and output variables. Variance errors are either of low\n",
    "variance or high variance.\n",
    "Low variance means there is a small variation in the prediction of the target function with changes in the \n",
    "training data set. At the same time, High variance shows a large variation in the prediction of the target\n",
    "function with changes in the training dataset.\n",
    "A model that shows high variance learns a lot and perform well with the training dataset, and does not\n",
    "generalize well with the unseen dataset. As a result, such a model gives good results with the training \n",
    "dataset but shows high error rates on the test dataset.\n",
    "Since, with high variance, the model learns too much from the dataset, it leads to overfitting of the model.\n",
    "A model with high variance has the below problems:\n",
    "o\tA high variance model leads to overfitting.\n",
    "o\tIncrease model complexities.\n",
    "Usually, nonlinear algorithms have a lot of flexibility to fit the model, have high variance.\n",
    "Some examples of machine learning algorithms with low variance are, Linear Regression, Logistic \n",
    "Regression, and Linear discriminant analysis. At the same time, algorithms with high variance are \n",
    "decision tree, Support Vector Machine, and K-nearest neighbours.\n",
    "Ways to Reduce High Variance:\n",
    "o\tReduce the input features or number of parameters as a model is overfitted.\n",
    "o\tDo not use a much complex model.\n",
    "o\tIncrease the training data.\n",
    "o\tIncrease the Regularization term.\n",
    "Different Combinations of Bias-Variance\n",
    "There are four possible combinations of bias and variances, which are represented by the below diagram:\n",
    " \n",
    "1.\tLow-Bias, Low-Variance:\n",
    "The combination of low bias and low variance shows an ideal machine learning model. However, it is not\n",
    "possible practically.\n",
    "2.\tLow-Bias, High-Variance: With low bias and high variance, model predictions are inconsistent and \n",
    "accurate on average. This case occurs when the model learns with a large number of parameters and hence\n",
    "leads to an overfitting\n",
    "3.\tHigh-Bias, Low-Variance: With High bias and low variance, predictions are consistent but inaccurate on\n",
    "average. This case occurs when a model does not learn well with the training dataset or uses few numbers of\n",
    "the parameter. It leads to underfitting problems in the model.\n",
    "4.\tHigh-Bias, High-Variance:\n",
    "With high bias and high variance, predictions are inconsistent and also inaccurate on average.\n",
    "How to identify High variance or High Bias?\n",
    "High variance can be identified if the model has:\n",
    " \n",
    "o\tLow training error and high test error.\n",
    "High Bias can be identified if the model has:\n",
    "o\tHigh training error and the test error is almost similar to training error.\n",
    "Bias-Variance Trade-Off\n",
    "While building the machine learning model, it is really important to take care of bias and variance\n",
    "in order to avoid overfitting and underfitting in the model. If the model is very simple with fewer\n",
    "parameters, it may have low variance and high bias. Whereas, if the model has a large number of parameters, it will have high variance and low bias. So, it is required to make a balance between bias and variance errors, and this balance between the bias error and variance error is known as the Bias-Variance trade-off.\n",
    " \n",
    "For an accurate prediction of the model, algorithms need a low variance and low bias. But this is not\n",
    "possible because bias and variance are related to each other:\n",
    "o\tIf we decrease the variance, it will increase the bias.\n",
    "o\tIf we decrease the bias, it will increase the variance.\n",
    "Bias-Variance trade-off is a central issue in supervised learning. Ideally, we need a model that\n",
    "accurately captures the regularities in training data and simultaneously generalizes well with \n",
    "the unseen dataset. Unfortunately, doing this is not possible simultaneously. Because a high variance\n",
    "algorithm may perform well with training data, but it may lead to overfitting to noisy data. Whereas,\n",
    "high bias algorithm generates a much simple model that may not even capture important regularities in \n",
    "the data. So, we need to find a sweet spot between bias and variance to make an optimal model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec2ca17-7254-4bd0-8e8c-5324bde84d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? \n",
    "Describe some common regularization techniques and how they work.\n",
    "\n",
    "Regularization in machine learning is the process of regularizing the parameters that constrain,\n",
    "regularizes, or shrinks the coefficient estimates towards zero. In other words, this technique\n",
    "discourages learning a more complex or flexible model, avoiding the risk of Overfitting.\n",
    "Regularization is a technique used in machine learning and deep learning to prevent overfitting \n",
    "and improve the generalization performance of a model. It involves adding a penalty term to the\n",
    "loss function during training\n",
    "Regularization in machine learning prevents the model from overfitting. It basically reduces or\n",
    "regularizes the coefficient of features towards zero. As mentioned above, regularization in machine \n",
    "learning, refers to a set of techniques that help the machine to learn more than just memorize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fea1c8-7ab6-43ff-8a39-a50460d2cf01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

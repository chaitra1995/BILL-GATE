{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ab2e72-fd96-4896-89a7-db74e281385e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. Explain the concept of R-squared in linear regression models. How is it calculated,\n",
    "and what does it represent?\n",
    "R-Squared (R² or the coefficient of determination) is a statistical measure in a regression\n",
    "model that determines the proportion of variance in the dependent variable that can be\n",
    "explained by the independent variable. In other words, r-squared shows how well the data \n",
    "fit the regression model (the goodness of fit).\n",
    "Formula for R-Squared\n",
    "\n",
    "The calculation of R-squared requires several steps. This includes taking the data points\n",
    "(observations) of dependent and independent variables and finding the line of best fit, often \n",
    "from a regression model. From there, you would calculate predicted values, subtract actual\n",
    "values, and square the results. This yields a list of errors squared, which is then summed\n",
    "and equals the unexplained variance.\n",
    "To calculate the total variance, you would subtract the average actual value from each of\n",
    "the actual values, square the results, and sum them. From there, divide the first sum of\n",
    "errors (unexplained variance) by the second sum (total variance), subtract the result\n",
    "from one, and you have the R-squared. \n",
    "\n",
    "\n",
    "Q2. Define adjusted R-squared and explain how it differs from the regular R-squared.\n",
    "Adjusted R-squared is a modified version of R-squared that has been adjusted for the number\n",
    "of predictors in the model. The adjusted R-squared increases when the new term improves the\n",
    "model more than would be expected by chance. It decreases when a predictor improves the model\n",
    "by less than expected.\n",
    "Adjusted R-squared, a modified version of R-squared, adds precision and reliability by considering\n",
    "the impact of additional independent variables that tend to skew the results of R-squared measurements.\n",
    "\n",
    "\n",
    "Q3. When is it more appropriate to use adjusted R-squared?\n",
    "Using adjusted R-squared over R-squared may be favored because of its ability to make a more \n",
    "accurate view of the correlation between one variable and another. Adjusted R-squared does\n",
    "this by taking into account how many independent variables are added to a particular model\n",
    "against which the stock index is measured.\n",
    "\n",
    "\n",
    "\n",
    "Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics\n",
    "calculated, and what do they represent?\n",
    "Mean absolute Error (MAE)\tMean square Error (MSE)\tRoot mean square error (RMSE)\n",
    "It doesn't account for the direction of the value. Even if value is negative, positive value\n",
    "is used for calculation.\tIt does account for positive or negative value.\tIt does account\n",
    "for positive or negative value.\n",
    "\n",
    "Mean absolute error (MAE)\n",
    "In simple terms, mean absolute error is the sum of absolute/positive errors of all values.\n",
    "So, if there are 5 values in our data set, we find out the difference between the actual value\n",
    "and predicted values for all 5 values and take their positive value. So even if the difference \n",
    "between actual and predicted value is negative, we take positive value for calculation.\n",
    "So we take the positive value of all errors, add them and find out their mean.\n",
    "Mean absolute error illustration;\n",
    "Actual Value (y)\tPredicted Value (y hat)\tError (difference)\tAbsolute Error\t\n",
    "100\t130\t-30\t30\t\n",
    "150\t170\t-20\t20\t\n",
    "200\t220\t-20\t20\t\n",
    "250\t260\t-10\t10\t\n",
    "300\t325\t-25\t25\t\n",
    "\t\t\t21\tMean\n",
    "Note- You take the absolute value of error which is the positive value, therefore -30 becomes 30\t\n",
    "\t\n",
    "\n",
    "MAE is the sum of absolute differences between actual and predicted values. It doesn’t consider the \n",
    "direction, that is, positive or negative.\n",
    "When we consider directions also, that is called Mean Bias Error (MBE), which is a sum of errors(difference).\n",
    "Formula for mean absolute Error or MAE is represented by;\n",
    " \n",
    "\n",
    "Mean Square Error (MSE)\n",
    "Mean square error is always positive and a value closer to 0 or a lower value is better. Let’s \n",
    "see how this this is calculated;\n",
    " \n",
    "Let’s use the last illustration to understand it better.\n",
    "Actual Value (y)\n",
    "\tPredicted Value (y hat)\n",
    "\tError (difference)\n",
    "\tSquared Error\n",
    "\t\n",
    "100\t130\t-30\t900\t\n",
    "150\t170\t-20\t400\t\n",
    "200\t220\t-20\t400\t\n",
    "250\t260\t-10\t100\t\n",
    "300\t325\t-25\t625\t\n",
    "\t\t\t485\tMean\n",
    "So if we were to run a model with different parameters/independent variables, model with lower \n",
    "MSE will be deemed better.\n",
    "We will look at its comparison with other loss functions in a while in this post. First quickly cover RMSE.\n",
    "Root mean square error (RMSE)\n",
    "Square root of MSE yields root mean square error (RMSE). So it’s formula is quite \n",
    "similar to what you have seen with mean square error, it’s just that we need to add a square root sign to it;\n",
    " \n",
    "It is the standard deviation of error (residual error).\n",
    "it indicates the spread of the residual errors. It is always positive, and a lower \n",
    "value indicates better performance. Ideal value would be 0 but it is never achieved.\n",
    "Actual Value (y)\tPredicted Value (y hat)\tError (difference)\tSquared Error\t\n",
    "100\t130\t-30\t900\t\n",
    "150\t170\t-20\t400\t\n",
    "200\t220\t-20\t400\t\n",
    "250\t260\t-10\t100\t\n",
    "300\t325\t-25\t625\t\n",
    "\t\t\t485\tMean\n",
    "\t\t\t22.02271555\tSquare root of mean\n",
    "Effect of each error on RMSE is directly proportional to the squared error therefore, \n",
    "RMSE is sensitive to outliers and can exaggerate results if there are outliers in the data set.\n",
    "Before moving to their comparison, I just want to mention one more evaluation metric and \n",
    "that is Root mean squared log error (RMSLE)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics \n",
    "in regression analysis.\n",
    "Metric\tAdvantages\tDisadvantage\n",
    "MAE\tEasy to interpret and understand less sensitive to outliers.\tDoes not take into account\n",
    "the directions of errors.\n",
    "MSE\tPenalizes larger  errors more  heavily ,giving it  more sensitivity to outliers.\t\n",
    "Harder to interpret than  MAE, as it is not  in the same unit  as the original data.\n",
    "RMSE\tHas the same units  as the original data, making  it easier \n",
    "To interpret.\tSensitive  to outliers.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization,\n",
    "and when is it more appropriate to use?\n",
    "The LASSO method regularizes model parameters by shrinking the regression coefficients,\n",
    "reducing some of them to zero. The feature selection phase occurs after the shrinkage, \n",
    "where every non-zero value is selected to be used in the model.\n",
    "Lasso regression, commonly referred to as L1 regularization, is a method for stopping\n",
    "overfitting in linear regression models by including a penalty term in the cost function. \n",
    "In contrast to Ridge regression, it adds the total of the absolute values of the coefficients\n",
    "rather than the sum of the squared coefficients.\n",
    "Lasso tends to do well if there are a small number of significant parameters and the others\n",
    "are close to zero (ergo: when only a few predictors actually influence the response).\n",
    "Ridge works well if there are many large parameters of about the same value\n",
    "(ergo:when most predictors impact the response)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q7. How do regularized linear models help to prevent overfitting in machine learning?\n",
    "Provide an example to illustrate.\n",
    "Regularization is a technique that penalizes the coefficient. In an overfit model,\n",
    "the coefficients are generally inflated. Thus, Regularization adds penalties to the\n",
    "parameters and avoids them weigh heavily. The coefficients are added to the cost\n",
    "function of the linear equation.\n",
    "Regularization refers to techniques that are used to calibrate machine learning models \n",
    "in order to minimize the adjusted loss function and prevent overfitting or underfitting. \n",
    "Using Regularization, we can fit our machine learning model appropriately on a given test \n",
    "set and hence reduce the errors in it.\n",
    "\n",
    "\n",
    "\n",
    "Q8. Discuss the limitations of regularized linear models and explain why they may not \n",
    "always be the best choice for regression analysis.\n",
    "Regularization is a way of controlling the complexity of a linear regression model, by\n",
    "penalizing the coefficients that are not important or relevant for the prediction.\n",
    "By doing so, regularization can reduce the variance of the model, as it prevents\n",
    "overfitting and makes the model more robust to noise and outliers\n",
    "\n",
    "The choice of regression model depends on the nature of the data and the problem at \n",
    "hand. Linear regression is often a good choice when the relationship between the variables\n",
    "is linear, but other models may be more appropriate for nonlinear relationships or when \n",
    "there are interactions between the variables.\n",
    "By understanding these frequently asked questions about linear regression, you can be\n",
    "better prepared for a machine learning interview and have a better understanding of how\n",
    "linear regression can be used to make predictions and solve problems in various fields.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q9. You are comparing the performance of two regression models using different evaluation metrics.\n",
    "Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better\n",
    "performer, and why? Are there any limitations to your choice of metric?\n",
    "Both RMSE (Root Mean Square Error) and MAE (Mean Absolute Error) are popular metrics for evaluating\n",
    "the performance of regression models, but they emphasize different aspects of prediction accuracy.\n",
    "Let’s analyze the situation using both metrics:\n",
    "Model A: RMSE = 10\n",
    "Model B: MAE = 8\n",
    "The choice of which model is better depends on the specific context and the importance of different\n",
    "aspects of prediction accuracy:\n",
    "- RMSE: RMSE gives more weight to larger errors due to the squaring operation. \n",
    "This can be advantageous when significant errors are particularly undesirable.\n",
    "However, RMSE is sensitive to outliers and can be influenced by extreme values,\n",
    "which might not reflect the overall model performance accurately.\n",
    "•\tMAE: MAE represents the average magnitude of errors, giving equal weight to all errors.\n",
    "It is less sensitive to outliers and can provide a more robust assessment of overall prediction accuracy.\n",
    "Comparing the two models:\n",
    "- Model A has a lower RMSE (10), indicating that it might perform better when large errors\n",
    "are of particular concern.\n",
    "- Model B has a lower MAE (8), suggesting that it might perform better in terms of overall\n",
    "average prediction accuracy.\n",
    "Since both RMSE and MAE have their strengths and weaknesses, the choice of the better\n",
    "model depends on the specific goals of the analysis and the relative importance of\n",
    "minimizing large errors versus achieving a more balanced average prediction accuracy.\n",
    "Limitations of Metric Choice:\n",
    "1. Domain Context: The choice of metric should consider the context of the problem.\n",
    "Depending on the domain and the consequences of different types of errors, one metric\n",
    "might be more appropriate than the other.\n",
    "2. Outlier Sensitivity: RMSE can be strongly affected by outliers due to the squaring operation.\n",
    "If the dataset contains outliers, RMSE might overemphasize their impact.\n",
    "3. Scale Considerations: RMSE and MAE are in different units than the original variable.\n",
    "Comparing models based solely on these metrics might not provide the full picture if the \n",
    "scales of the variables are different.\n",
    "4. Model Goal: The ultimate goal of the model also matters. If the model is intended for\n",
    "decision-making, cost considerations, or specific accuracy requirements, the choice of metric\n",
    "should align with those goals.\n",
    "5. Model Complexity: Metrics should be interpreted alongside model complexity. A simpler model\n",
    "with slightly higher errors might still be preferable due to its interpretability and generalization.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q10. You are comparing the performance of two regularized linear models using different types\n",
    "of regularization. Model A uses Ridge regularization with a regularization parameter of 0.1,\n",
    "while Model B uses Lasso regularization with a regularization parameter of 0.5. Which model\n",
    "would you choose as the better performer, and why? Are there any trade-offs or limitations \n",
    "to your choice of regularization\n",
    "method?\n",
    "In order to create less complex (parsimonious) model when you have a large number of features\n",
    "in your dataset, some of the Regularization techniques used to address over-fitting and feature selection are:\n",
    "1. L1 Regularization\n",
    "2. L2 Regularization\n",
    "A regression model that uses L1 regularization technique is called Lasso Regression and model \n",
    "which uses L2 is called Ridge Regression.\n",
    "The key difference between these two is the penalty term.\n",
    "\n",
    "Ridge regression adds “squared magnitude” of coefficient as penalty term to the loss function.\n",
    "Here the highlighted part represents L2 regularization element.\n",
    " \n",
    "Cost function\n",
    "Here, if lambda is zero then you can imagine we get back OLS. However, if lambda is very large \n",
    "then it will add too much weight and it will lead to under-fitting. Having said that it’s important\n",
    "how lambda is chosen. This technique works very well to avoid over-fitting issue.\n",
    "Lasso Regression (Least Absolute Shrinkage and Selection Operator) adds “absolute value of magnitude”\n",
    "of coefficient as penalty term to the loss function.\n",
    " \n",
    "Cost function\n",
    "Again, if lambda is zero then we will get back OLS whereas very large value will make coefficients\n",
    "zero hence it will under-fit.\n",
    "The key difference between these techniques is that Lasso shrinks the less important feature’s \n",
    "coefficient to zero thus, removing some feature altogether. So, this works well for feature selection\n",
    "in case we have a huge number of features.\n",
    "Traditional methods like cross-validation, stepwise regression to handle overfitting and perform\n",
    "feature selection work well with a small set of features but these techniques are a great alternative \n",
    "when we are dealing with a large set of features\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

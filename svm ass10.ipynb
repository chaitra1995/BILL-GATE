{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73a7c2e-3c80-467f-aeb1-1d423736d12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. A company conducted a survey of its employees and found that 70% of the employees use the company's\n",
    "health insurance plan, while 40% of the employees who use the plan are smokers. What is the probability\n",
    "that an employee is a smoker given that he/she uses the health insurance plan?\n",
    "\n",
    "Smokers employee/employee  =0.4/0.7=0.571.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e64e90-a054-493d-8434-22b440a46166",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?\n",
    "The Multinomial Naive Bayes model takes into account each occurrence of a token, whereas\n",
    "the Bernoulli Naive Bayes model does not (i.e. for the latter model, 3 occurrences of \"viagra\"\n",
    "is the same as 1 occurrence of \"viagra\").\n",
    "Here are two illustrations as well as a comparison table from {1}:\n",
    "\n",
    "{1} neatly introduces Naive Bayes for text classification, as well as the Multinomial Naive Bayes model and \n",
    "the Bernoulli Naive Bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3a40e0-5371-4865-a26c-19187105f333",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. How does Bernoulli Naive Bayes handle missing values?\n",
    "\n",
    "Naive Bayes can handle missing data. Attributes are handled separately by the algorithm at both model\n",
    "construction time and prediction time. As such, if a data instance has a missing value for an attribute,\n",
    "it can be ignored while preparing the model, and ignored when a probability is calculated for a class value\n",
    "Na√Øve Bayes Imputation (NBI) is used to fill in missing values by replacing the attribute information\n",
    "according to the probability estimate. The NBI process divides the whole data into two sub-sets is the \n",
    "complete data and data containing missing data. Complete data is used for the imputation process at the lost value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc96d08-405d-4fda-b3ed-fce90a902081",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. Can Gaussian Naive Bayes be used for multi-class classification?\n",
    "The Naive Bayes algorithm can be used for multi-class classification problems with more than two classes. \n",
    "There are several types of Naive Bayes classifiers, such as Multinomial Naive Bayes, Bernoulli Naive Bayes,\n",
    "and Gaussian Naive Bayes\n",
    "It is based on the Bayes' Theorem for calculating probabilities and conditional probabilities. \n",
    "You can use it for real-time and multi-class predictions, text classifications, spam filtering, sentiment\n",
    "analysis, and a lot more.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7500214f-9bbe-4dab-ac3a-dd89dd69f181",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. Assignment:\n",
    "Data preparation:\n",
    "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/\n",
    "datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message\n",
    "is spam or not based on several input features.\n",
    "Implementation:\n",
    "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the dataset. You should use the default hyperparameters for each classifier.\n",
    "\n",
    "Results:\n",
    "Report the following performance metrics for each classifier:\n",
    "Accuracy\n",
    "Precision\n",
    "Recall\n",
    "F1 score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bce077f-e940-456b-84ba-ef6508ff7138",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "print(\"Scikit-Learn Version : {}\".format(sklearn.__version__))\n",
    "np.set_printoptions(precision=2)\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.datasets import load_boston, load_digits\n",
    "digits = load_digits()\n",
    "X_digits, Y_digits = digits.data, digits.target\n",
    "print('Dataset Size : ', X_digits.shape, Y_digits.shape)\n",
    "\n",
    "n_features, n_classes = X_digits.shape[1], np.unique(Y_digits)\n",
    "n_features, n_classes\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_digits, Y_digits, train_size=0.80,\n",
    "                                                    test_size=0.20, stratify=Y_digits, random_state=123)\n",
    "print('Train/Test Sizes : ', X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\n",
    "\n",
    "X_digits.min(), X_digits.max()\n",
    "\n",
    "fig = plt.figure(figsize=(12,5))\n",
    "plt.imshow(np.hstack(X_digits[:10].reshape(10,8,8)), cmap=\"gray\");\n",
    "plt.xticks([], []);\n",
    "plt.yticks([], []);\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "bernoulli_nb =  BernoulliNB()\n",
    "bernoulli_nb.fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "Y_preds = bernoulli_nb.predict(X_test)\n",
    "print(Y_preds[:15])\n",
    "print(Y_test[:15])\n",
    "print('\\nTest Accuracy : {:.3f}'.format(bernoulli_nb.score(X_test, Y_test))) ## Score method also evaluates accuracy for classification models.\n",
    "print('Training Accuracy : {:.3f}'.format(bernoulli_nb.score(X_train, Y_train)))\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "print(\"Accuracy Score : {:.3f}\".format(accuracy_score(Y_test, Y_preds)))\n",
    "print(\"\\nClassification Report :\")\n",
    "print(classification_report(Y_test, Y_preds))\n",
    "\n",
    "\n",
    "import scikitplot as skplt\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111)\n",
    "skplt.metrics.plot_confusion_matrix(Y_test, Y_preds,\n",
    "                                    normalize=True,\n",
    "                                    title=\"Confusion Matrix\",\n",
    "                                    cmap=\"Purples\",\n",
    "                                    ax=ax\n",
    "                                    );\n",
    "\n",
    "\n",
    "bernoulli_nb.class_log_prior_\n",
    "\n",
    "\n",
    "print(\"Log Probability of Each Feature per class : \", bernoulli_nb.feature_log_prob_.shape)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "params = {'alpha': [0.01, 0.1, 0.5, 1.0, 10.0],\n",
    "          'fit_prior': [True, False],\n",
    "          'class_prior': [None, [0.1,]* len(n_classes), ],\n",
    "          'binarize': [None, 0.0, 8.5, 10.0]\n",
    "         }\n",
    "\n",
    "bernoulli_nb_grid = GridSearchCV(BernoulliNB(), param_grid=params, n_jobs=-1, cv=5, verbose=5)\n",
    "bernoulli_nb_grid.fit(X_train,Y_train)\n",
    "print('Best Parameters : {}'.format(bernoulli_nb_grid.best_params_))\n",
    "print('Best Accuracy Through Grid Search : {:.3f}\\n'.format(bernoulli_nb_grid.best_score_))\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "Y_preds = bernoulli_nb_grid.best_estimator_.predict(X_test)\n",
    "Y_preds_train = bernoulli_nb_grid.best_estimator_.predict(X_train)\n",
    "print(\"Test Accuracy Score : {:.3f}\".format(accuracy_score(Y_test, Y_preds)))\n",
    "print(\"Train Accuracy Score : {:.3f}\".format(accuracy_score(Y_train, Y_preds_train)))\n",
    "print(\"\\nClassification Report :\")\n",
    "print(classification_report(Y_test, Y_preds))\n",
    "\n",
    "\n",
    "import scikitplot as skplt\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111)\n",
    "skplt.metrics.plot_confusion_matrix(Y_test, Y_preds,\n",
    "                                    normalize=True,\n",
    "                                    title=\"Confusion Matrix\",\n",
    "                                    cmap=\"Purples\",\n",
    "                                    ax=ax\n",
    "                                    );\n",
    "\n",
    "gaussian_nb.class_prior_\n",
    "\n",
    "gaussian_nb.epsilon_\n",
    "\n",
    "print(\"Gaussian Naive Bayes Sigma Shape : {}\".format(gaussian_nb.var_.shape))\n",
    "\n",
    "\n",
    "print(\"Gaussian Naive Bayes Theta Shape : {}\".format(gaussian_nb.theta_.shape))\n",
    "\n",
    "\n",
    "params = {\n",
    "            'priors': [None, [0.1,]* len(n_classes),],\n",
    "            'var_smoothing': [1e-9, 1e-6, 1e-12],\n",
    "         }\n",
    "\n",
    "gaussian_nb_grid = GridSearchCV(GaussianNB(), param_grid=params, n_jobs=-1, cv=5, verbose=5)\n",
    "gaussian_nb_grid.fit(X_train,Y_train)\n",
    "\n",
    "print('Best Accuracy Through Grid Search : {:.3f}'.format(gaussian_nb_grid.best_score_))\n",
    "print('Best Parameters : {}\\n'.format(gaussian_nb_grid.best_params_))\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "Y_preds = gaussian_nb_grid.best_estimator_.predict(X_test)\n",
    "Y_preds_train = gaussian_nb_grid.best_estimator_.predict(X_train)\n",
    "\n",
    "print(\"Test Accuracy Score : {:.3f}\".format(accuracy_score(Y_test, Y_preds)))\n",
    "print(\"Train Accuracy Score : {:.3f}\".format(accuracy_score(Y_train, Y_preds_train)))\n",
    "print(\"\\nClassification Report :\")\n",
    "print(classification_report(Y_test, Y_preds))\n",
    "\n",
    "import scikitplot as skplt\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111)\n",
    "skplt.metrics.plot_confusion_matrix(Y_test, Y_preds,\n",
    "                                    normalize=True,\n",
    "                                    title=\"Confusion Matrix\",\n",
    "                                    cmap=\"Purples\",\n",
    "                                    ax=ax\n",
    "                                    );\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Discussion:\n",
    "Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think \n",
    "that is the case? Are there any limitations of Naive Bayes that you observed?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f1054e-dd6f-484b-a481-a5971de24030",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289d50c9-c947-4e9e-b6ce-6da059e40a9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

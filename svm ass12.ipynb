{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe8fa60-d71e-443f-bada-336b740f531a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. How does bagging reduce overfitting in decision trees?\n",
    "\n",
    "Ensemble methods are another way to avoid overfitting in decision trees, as they can combine multiple\n",
    "trees into a single model, and improve the accuracy and robustness of the model. Ensemble methods can \n",
    "be applied by using different algorithms or techniques, such as bagging, boosting, or stacking\n",
    "By training models on different bootstraps, bagging reduces the variance of the individual models. \n",
    "It also avoids overfitting by exposing the constituent models to different parts of the dataset.\n",
    "The predictions from all the sampled models are then combined through a simple averaging to make\n",
    "the overall prediction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8ee022-75d1-4a52-b8cf-843286b52b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. What are the advantages and disadvantages of using different types of base learners in bagging?\n",
    "1.Easier for implementation :Python libraries, including scikit-examine (sklearn), make it easy to \n",
    "mix the predictions of base beginners or estimators to enhance model performance.\n",
    "Their documentation outlines the available modules you can leverage for your model optimization.\n",
    "2. Variance reduction :The Bagging can reduce the variance inside a getting to know set \n",
    "of rules which is especially helpful with excessive-dimensional facts, where missing values\n",
    "can result in better conflict, making it more liable to overfitting and stopping correct\n",
    "generalization to new datasets.\n",
    "There are many disadvantages of Bagging. The disadvantages of Bagging are given below -\n",
    "1. Flexible less:\n",
    "As a method, Bagging works particularly correctly with algorithms that are much less solid.\n",
    "One which can be more stable or a problem with high amounts of bias does now not provide an awful \n",
    "lot of gain as there is less variation in the dataset of the version. As noted within the hands-On \n",
    "guide for machine learning, \"the bagging is a linear regression version will efficaciously just return \n",
    "the original predictions for huge enough b.\"\n",
    "2. Loss of interpretability:\n",
    "The Bagging slows down and grows extra in depth because of the quantity of iterations growth. accordingly, \n",
    "it is no longer adequately suitable for actual-time applications. Clustered structures or large processing\n",
    "cores are perfect for quickly growing bagged ensembles on massive look-at units.\n",
    "3. Expensive for computation:\n",
    "The Bagging is tough to draw unique business insights via Bagging because of the averaging concerned \n",
    "throughout predictions. While the output is more precise than any person's information point, a more \n",
    "accurate or whole dataset may yield greater precision within a single classification or regression model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca71b4b6-ff39-4ed1-b7b5-e77766ded2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. How does the choice of base learner affect the bias-variance tradeoff in bagging?\n",
    "In machine learning, no matter if we are facing a classification or a regression problem, the choice \n",
    "of the model is extremely important to have any chance to obtain good results. This choice can depend \n",
    "on many variables of the problem: quantity of data, dimensionality of the space, distribution hypothesis…A\n",
    "low bias and a low variance, although they most often vary in opposite directions, are the two most\n",
    "fundamental features expected for a model. Indeed, to be able to “solve” a problem, we want our model \n",
    "to have enough degrees of freedom to resolve the underlying complexity of the data we are working with,\n",
    "but we also want it to have not too much degrees of freedom to avoid high variance and be more robust. \n",
    "This is the well known bias-variance tradeoff. In ensemble learning theory, we call weak learners\n",
    "(or base models) models that can be used as building blocks for designing more complex models by\n",
    "combining several of them. Most of the time, these basics models perform not so well by themselves \n",
    "either because they have a high bias (low degree of freedom models, for example) or because they have\n",
    "too much variance to be robust (high degree of freedom models, for example). Then, the idea of ensemble\n",
    "methods is to try reducing bias and/or variance of such weak learners by combining several of them together \n",
    "in order to create a strong learner (or ensemble model) that achieves better performances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20b24b3-2597-47b7-b9ba-5f1fb64053f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. Can bagging be used for both classification and regression tasks? How does it differ in each case?\n",
    "The bagging technique is useful for both regression and statistical classification. Bagging is used \n",
    "with decision trees, where it significantly raises the stability of models in improving accuracy and\n",
    "reducing variance, which eliminates the challenge of overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa8b710-d44c-4500-bf34-90126e785b16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef02163-0d80-4674-8c66-060c933c29d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cdb65a-f59f-4cbb-a589-d5b7ea1a4dc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba8c1c0-ddc4-461f-b697-14b51b58e998",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

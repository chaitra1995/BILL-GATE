{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b152e2f1-f2f3-427e-b487-a227762d85ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is the mathematical formula for a linear SVM?\n",
    "The equation of a hyperplane is w.x+b=0 where w is a vector normal to hyperplane and b is an offset. \n",
    "If the value of w.x+b>0 then we can say it is a positive point otherwise it is a negative point.\n",
    "Now we need (w,b) such that the margin has a maximum distance.\n",
    "Consider a binary classification problem with two classes, labeled as +1 and -1. We have a training \n",
    "dataset consisting of input feature vectors X and their corresponding class labels Y.\n",
    "\n",
    "The equation for the linear hyperplane can be written as:\n",
    "\n",
    "w^Tx+ b = 0\n",
    "\n",
    "The vector W represents the normal vector to the hyperplane. i.e the direction perpendicular\n",
    "to the hyperplane. The parameter b in the equation represents the offset or distance of the\n",
    "hyperplane from the origin along the normal vector w. \n",
    "\n",
    "The distance between a data point x_i and the decision boundary can be calculated as:\n",
    "\n",
    "d_i = \\frac{w^T x_i + b}{||w||}\n",
    "\n",
    "where ||w|| represents the Euclidean norm of the weight vector w. Euclidean norm of the normal vector W\n",
    "\n",
    "For Linear SVM classifier :\n",
    "\n",
    "\\hat{y} = \\left\\{ \\begin{array}{cl} 1 & : \\ w^Tx+b \\geq 0 \\\\ 0 & : \\  w^Tx+b  < 0 \\end{array} \\right.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2f24c5-d615-465a-8e4c-82e45ae441ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. What is the objective function of a linear SVM?\n",
    "The objective of the SVM algorithm is to find a hyperplane that, to the best degree possible,\n",
    "separates data points of one class from those of another class. “Best” is defined as the hyperplane \n",
    "with the largest margin between the two classes, represented by plus versus minus in the figure below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf98433-d569-4c69-9bfb-6274c7fa1351",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Q3. What is the kernel trick in SVM?\n",
    "The “Kernel Trick” is a method used in Support Vector Machines (SVMs) to convert data\n",
    "(that is not linearly separable) into a higher-dimensional feature space where it may be linearly separated\n",
    "SVM has a technique called the kernel trick. These are functions that take low dimensional \n",
    "input space and transform it into a higher-dimensional space i.e. it converts not separable\n",
    "problem to separable problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bfb801-3be4-414e-b3fe-496b27bdb14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What is the role of support vectors in SVM Explain with example\n",
    "Support Vectors: These are the points that are closest to the hyperplane. A separating line will\n",
    "be defined with the help of these data points. Margin: it is the distance between the hyperplane \n",
    "and the observations closest to the hyperplane (support vectors). In SVM large margin is considered \n",
    "a good margin.\n",
    "SVMs are used in applications like handwriting recognition, intrusion detection, face detection,\n",
    "email classification, gene classification, and in web pages. This is one of the reasons we use\n",
    "SVMs in machine learning. It can handle both classification and regression on linear and non-linear data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860a7458-fe58-47bf-a3e8-fbe719d2a26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin and Hard margin in SVM?\n",
    "Hyperplanes are essentially a boundary which classifies the data set (classifies Spam email from the ham ones).\n",
    "It could be lines, 2D planes, or even n-dimensional planes that are beyond our imagination.\n",
    "A line that is used to classify one class from another is called a hyperplane.\n",
    "Hyperplanes are decision boundaries that help classify the data points. Data points falling on \n",
    "either side of the hyperplane can be attributed to different classes. \n",
    "Also, the dimension of the hyperplane depends upon the number of features. \n",
    "If the number of input features is 2, then the hyperplane is just a line.\n",
    "If the number of input features is 3, then the hyperplane becomes a two-dimensional plane.\n",
    "It becomes difficult to imagine when the number of features exceeds 3.In a p-dimensional space,\n",
    "a hyperplane is a flat affine subspace of dimension p-1. Visually, in a 2D space, the hyperplane\n",
    "will be a line, and in a 3D space, it will be a flat plane.\n",
    "Mathematically, the hyperplane is simply:\n",
    "In general, if the data can be perfectly separated using a hyperplane, then there is an infinite number\n",
    "of hyperplanes, since they can be shifted up or down, or slightly rotated without coming into contact\n",
    "with an observation.That is why we use the maximal margin hyperplane or optimal separating hyperplane\n",
    "which is the separating hyperplane that is farthest from the observations. \n",
    "We calculate the perpendicular distance from each training observation given a hyperplane. \n",
    "This is known as the margin. Margin is defined as the gap between two lines on the closet data points\n",
    "of different classes. It can be calculated as the perpendicular distance from the line to the support\n",
    "vectors. Large margin is considered as a good margin and small margin is considered as a bad margin.\n",
    "Support Vectors are datapoints that are closest to the hyperplane. Separating line will be defined\n",
    "with the help of these data points.\n",
    "\n",
    "Hard  Margin\n",
    "Let?s assume that the hyperplane separating our two classes is defined as  :\n",
    " \n",
    "Then, we can define the margin by two parallel hyperplanes:\n",
    "   \n",
    "   \n",
    "They are the green and purple lines in the above figure. Without allowing any misclassifications in \n",
    "the hard margin SVM, we want to maximize the distance between the two hyperplanes. To find this distance,\n",
    "we can use the formula for the distance of a point from a plane. So the distance of the blue points and\n",
    "the red point from the black line would respectively be:\n",
    "   \n",
    "As a result, the total margin would become:\n",
    "   \n",
    "We want to maximize this margin. Without the loss of generality, we can consider   and  . Subsequently, \n",
    "the problem would be to maximize   or minimize  . To make the problem easier when taking the gradients,\n",
    "we’ll, instead, word with its squared form:\n",
    "   \n",
    "This optimization comes with some constraints. Let?s assume that the labels for our classes are {-1, +1}.\n",
    "When classifying the data points, we want the points belonging to positives classes to be greater than  ,\n",
    "    meaning  , and the points belonging to the negative classes to be less than  , i.e.  .\n",
    "We can combine these two constraints and express them as:  . Therefore our optimization problem would become:\n",
    "   \n",
    "   \n",
    "This optimization is called the primal problem and is guaranteed to have a global minimum.\n",
    "We can solve this by introducing Lagrange multipliers ( ) and converting it to the dual problem:\n",
    "   \n",
    "This is called the Lagrangian function of the SVM which is differentiable with respect to   and b.\n",
    "   \n",
    "   \n",
    "By substituting them in the second term of the Lagrangian function, we?ll get the dual problem of SVM:\n",
    "   \n",
    "The dual problem is easier to solve since it has only the Lagrange multipliers. \n",
    "Also, the  fact that the dual problem depends on the inner products of the training\n",
    "data comes in handy when extending linear SVM to learn non-linear boundaries.\n",
    "\n",
    "Soft margin\n",
    "The soft margin SVM follows a somewhat similar optimization procedure with a couple of differences.\n",
    "First, in this scenario, we allow misclassifications to happen.\n",
    "So we’ll need to minimize the misclassification error, which means that we?ll have to deal\n",
    "with one more constraint. Second, to minimize the error, we should define a loss function.\n",
    "A common loss function used for soft margin is the hinge loss.\n",
    "   \n",
    "The loss of a misclassified point is called a slack variable and is added to the primal problem\n",
    "that we had for hard margin SVM. So the primal problem for the soft margin becomes:\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7527e412-11f4-4aae-8da4-24f7a1566220",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. SVM Implementation through Iris dataset.\n",
    "\n",
    "Bonus task: Implement a linear SVM classifier from scratch using Python and compare its\n",
    "performance with the scikit-learn implementation.\n",
    "~ Load the iris dataset from the scikit-learn library and split it into a training set and a testing set.\n",
    "~ Train a linear SVM classifier on the training set and predict the labels for the testing setl\n",
    "~ Compute the accuracy of the model on the testing setl\n",
    "~ Plot the decision boundaries of the trained model using two of the featuresl\n",
    "~ Try different values of the regularisation parameter C and see how it affects the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4cf2d7f-ef73-4307-9f2c-998d5f6e8f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import svm, datasets\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a51690f-881a-4fe5-8b0f-fc603d759327",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
